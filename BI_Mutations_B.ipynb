{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zldSmmvmLF5T"
   },
   "source": [
    "# Finding Mutations\n",
    "# B. Finding Mutations in DNA and Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBvEUsDuLF5U"
   },
   "source": [
    "Code: Nhan TV.<br>\n",
    "Email: TaVanNhan_sdh@hus.edu.vn.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrHAPWGHLF5V"
   },
   "source": [
    "Compeau, Phillip. <i>Bioinformatics Algorithms: An Active Learning Approach by Phillip Compeau, Pavel Pevzner (2014) Paperback.</i> La Jolla, CA: Active Learning Publishers, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSY2GYnpLF5W"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9967ZhmLF5X"
   },
   "source": [
    "<a><img src=\"https://imgur.com/XwZtyQW.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kgz1y_E3LF5Y"
   },
   "source": [
    "## Table of Contents\n",
    "1 [Multiple Pattern Matching](#matching)<br>\n",
    "   &emsp;1.1 [Trie Construction](#trie-construction)<br>\n",
    "   &emsp;1.2 [Applying the Trie to Multiple Pattern Matching](#multiple-pattern-matching)<br>\n",
    "   &emsp;1.3 [Suffix Tries](#suffix-tries)<br>\n",
    "   &emsp;1.4 [Suffix Trees](#suffix-tree)<br>\n",
    "   &emsp;1.5 [Suffix Arrays](#suffix-arrays)<br>\n",
    "2 [Burrows-Wheeler Transform (BWT)](#bwt)<br>\n",
    "   &emsp;2.1 [Genome Compression](#genome-compression)<br>\n",
    "   &emsp;2.2 [Constructing the Burrows-Wheeler transform](#constructing-bwt)<br>\n",
    "   &emsp;2.3 [Inverting the Burrows-Wheeler Transform](#inverting)<br>\n",
    "   &emsp;2.4 [Pattern Matching with the Burrows-Wheeler Transform](#pattern-matching)<br>\n",
    "   &emsp;2.5 [Speeding Up Burrows-Wheeler Pattern Matching](#speed-up)<br>\n",
    "   &emsp;2.6 [Locating Matched Patterns with BWT](#locate-bwt)<br>\n",
    "   &emsp;2.7 [ Approximate pattern matching with the Burrows-Wheeler transform](#approximate)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Multiple Pattern Matching<a name = \"matching\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Nguyên nhân nào gây ra hội chứng Ohdo?</b><br><br>\n",
    "Gần 1% các đứa trẻ sinh ra bị thiểu năng trí tuệ, nhưng vấn đề này vẫn gây khó khăn cho các nhà sinh học bởi chúng được gây ra bởi các rối loạn về gen khác nhau. Một trong những rối loạn này là hội chứng gây ra sự diễn đạt kém và khuôn mặt giống như một chiếc mặt nạ được gọi là hội chứng Ohdo. Vào năm 2011, các nhà sinh học đã giải được câu đố này bằng cách khám phá ra một số lượng các đột biến giống nhau của nhiều bệnh nhân. Các nhà nghiên cứu đã xác định được các đột biến của các đoạn cắt protein đơn cho hội chứng Odho [9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khám phá về nguồn gốc của hội chứng Ohdo này là một trong nhiều khám phá được thực hiện nhờ sử dụng <b>read mapping</b> trong nghiên cứu các rối loạn về gen. Trong read mapping, các nhà nghiên cứu so sánh các DNA reads được giải trình tự từ các cá nhân với <b>bộ gen tham chiếu (reference human genome)</b> để tìm ra các reads khớp (match) hoàn toàn với bộ gen tham chiếu này và các reads có thể chứa các đột biến từ một nucleotide này sang một nucleotide khác (single nucleotide polymorphisms, hoặc SNPs). Ngoài khoảng 3 triệu SNP (0,1% bộ gen của con người), con người còn khác nhau bởi sự sắp xếp lại bộ gen (rearrangements), sự chèn thêm (insertions) và xóa (deletions) có thể mở rộng ra tới hàng ngàn nucleotide [10][11]. Tuy nhiên, toàn bộ phần này chỉ tập chung vào những thuật toán tìm kiếm các SNPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Giới thiệu về Multiple Pattern Matching</b><br><br>\n",
    "Như chúng ta đã biết, các cặp đọc có thể được cung cấp bởi máy giải trình tự gen. Các lần đọc sẽ tạo thành một tập hợp các chuỗi patterns mà chúng ta muốn khớp với bộ gen text. Đối với mỗi chuỗi trong patterns, trước tiên chúng ta sẽ tìm thấy tất cả các kết quả khớp chính xác của nó dưới dạng một chuỗi con của text (hoặc kết luận rằng nó không xuất hiện trong text). Khi đi tìm nguyên nhân của một rối loạn di truyền, chúng ta có thể loại bỏ ngay vùng xem xét khỏi các khu vực của bộ gen tham chiếu nơi match chính xác xảy ra. Sau đó, chúng ta sẽ khái quát bài toán này để tìm các kết quả gần đúng, trong đó có các nucleotide đơn bị thay thế trong các lần đọc so với bộ gen tham chiếu (hoặc đại diện cho lỗi đọc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách tiếp cận ngây thơ nhất cho bài toán Multiple Pattern Matching là thực hiện vòng lặp với thuật toán Pattern Matching, thuật toán này được gọi là Brute Force Pattern Matching. Ở đây, chúng ta sẽ trượt mỗi pattern dọc theo text, kiểm tra xem liệu có chuỗi con nào bắt đầu từ mỗi vị trí của text khớp với pattern hay không. Thời gian chạy của thuật toán này là $\\mathcal{O}(|Text|.|Pattern|)$, vì vậy runtime của Brute Force Pattern Matching sẽ là $\\mathcal{O}(|Text|.|Patterns|)$ trong đó |Text| là chiều dài của Text, còn |Patterns| là tổng chiều dài của tất cả các Pattern. Vấn đề với việc áp dụng thuật toán Brute Force Pattern Matching phát sinh khi |Text| và |Patterns| rất lớn. Trong trường hợp gen người (3GB), tổng chiều dài của toàn bộ các reads có thể vượt quá 1TB, dẫn đến bất kỳ thuật toán nào với thời gian chạy $\\mathcal{O}(|Text|.|Patterns|)$ đều trở nên quá chậm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Trie Construction<a name = \"trie-construction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian chạy của Brute Force Pattern Matching rất lâu là vì mỗi chuỗi trong Pattern phải đi qua tất cả Text một cách độc lập. Nếu bạn nghĩ về Text như một con đường dài, thì Brute Force Pattern Matching tương tự như tải từng pattern vào chiếc xe riêng của mình khi lái xuống Text, một chiến lược không hiệu quả. Thay vào đó, mục tiêu của chúng ta là đưa các mô hình lên xe buýt để chúng ta chỉ cần thực hiện một chuyến đi từ điểm bắt đầu đến cuối Text. Nói cách khác, chúng ta muốn tổ chức các Pattern vào cấu trúc dữ liệu để ngăn chặn nhiều đường truyền xuống Text và để giảm thời gian chạy.\n",
    "Cuối cùng, chúng ta sẽ hợp nhất các Patterns thành một đồ thị không chu trình có hướng được gọi là Trie (phát âm là \"Try\"), được viết là Trie(Patterns) có các thuộc tính sau:<br><br>\n",
    "&emsp;+) Trie có một nút gốc duy nhất với indegree 0, ký hiệu là root.<br>\n",
    "&emsp;+) Mỗi cạnh của Trie (Patterns) được dán nhãn bằng một chữ cái của bảng chữ cái.<br>\n",
    "&emsp;+) Mỗi chuỗi trong Pattern được đánh vần bằng cách ghép các chữ cái dọc theo một số đường dẫn\n",
    "từ gốc trở xuống.<br>\n",
    "&emsp;+) Mọi đường dẫn từ gốc đến lá hoặc nút có outdegree 0, đánh vần một chuỗi từ Patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/SPpldvA.jpg\" width=\"400\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all out-going edge from a node.\n",
    "def get_out_edge(node, graph):\n",
    "    all_out_edge = []\n",
    "    for edge in graph:\n",
    "        if node == edge[0]:\n",
    "            all_out_edge.append(edge)\n",
    "    return(all_out_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get end node of out-going edge and corresponding label.\n",
    "def get_end_node_label(current_node, all_out_edge):\n",
    "    if all_out_edge == []:\n",
    "        end_node_label = [[current_node, 'None']]\n",
    "    else:\n",
    "        end_node_label = []\n",
    "        \n",
    "        for edge in all_out_edge:\n",
    "            \"\"\"If there is a out-going edge from a node, \n",
    "            we will append end node of that edge and corresponding label\"\"\"\n",
    "            if current_node == edge[0]:\n",
    "                end_node_label.append([edge[1], edge[2]])\n",
    "            else:\n",
    "                end_node_label = [[current_node, 'None']]\n",
    "                \n",
    "    return(end_node_label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out-going node that its correspoding label is the same as current symbol.\n",
    "def get_out_going_node(end_node_label, current_node, current_symbol):\n",
    "    logic = []\n",
    "    current_node = []\n",
    "    # Finding out-going node that is end node of out-going edge \n",
    "    for end_node in end_node_label:        \n",
    "        if current_node != end_node[0] and current_symbol == end_node[1]:\n",
    "            current_node.append(end_node[0])\n",
    "            logic.append(\"True\")\n",
    "        else:\n",
    "            current_node.append(current_node)\n",
    "            logic.append(\"False\")\n",
    "    \"\"\"Logic = True if out-going node that its corresponding label is the same as current symbol,\n",
    "    else with logic = False\"\"\"        \n",
    "    if \"True\" in logic:\n",
    "        lg = True\n",
    "        node = current_node[logic.index(\"True\")]\n",
    "    else:\n",
    "        lg = False\n",
    "        node = 0\n",
    "        \n",
    "    return (lg, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a trie from Patterns.\n",
    "def construct_trie(patterns):\n",
    "    \n",
    "    trie = []\n",
    "    leaves = []\n",
    "    new_node = 0\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        current_node = 0\n",
    "        \n",
    "        for i in range(len(pattern)):\n",
    "            current_symbol = pattern[i] # Get current symbol.\n",
    "            all_out_edge = get_out_edge(current_node, trie) # Get all out-going edges.\n",
    "            # Get end nodes of out-going edges and corresponing labels\n",
    "            end_node_label = get_end_node_label(current_node, all_out_edge) \n",
    "            # Finding end node that its correspoding label and current symbol are the same.\n",
    "            lg, node = get_out_going_node(end_node_label, current_node, current_symbol)\n",
    "            # If logic =  True, only setting current node to end of out-going edge.\n",
    "            if lg == True: \n",
    "                current_node = node\n",
    "            # If logic = False, we add new edge to the trie and set current node to new node.\n",
    "            else:\n",
    "                new_node += 1\n",
    "                trie.append([current_node, new_node, current_symbol])\n",
    "                current_node = new_node\n",
    "        # Finfing leaves of the trie.\n",
    "        leaves.append(current_node)\n",
    "    return(trie, leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priting graph.\n",
    "def print_graph(graph):\n",
    "    for edge in graph:\n",
    "        string = (str(edge[0]), str(edge[1]))\n",
    "        print(\"->\".join(string) + \":\"+ edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/Data Science/Data/Mutations/How Do We Locate Disease/dataset_294_4 (2).txt\", sep=\" \", header=None)\n",
    "patterns = data.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  CGCGGCTTTTCAGGATTCAACACTTGTTTAGTTTCGGGTCTTAACA...\n",
      "1  GTAGTTTTATCAAGGCGTCTTAACCGATTCATTGGGCTAAACGCTT...\n",
      "2  ATCCAAACACCGACAATCAAGAGCCTCCGTAGCTCCCAGTGGCGAG...\n",
      "3  GCCGAGTGTGGATCGGAAATGAGTTTTCCTGTGCTCTCCTTTTTCC...\n",
      "4  CACAAGGCGTTCGTCTGTATACTGTTTGCTAGGCACAATGGTGTTC...\n",
      "(82, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie, leaves = construct_trie(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183->184:A\n",
      "184->185:G\n",
      "185->186:G\n",
      "0->187:A\n",
      "187->188:T\n",
      "188->189:C\n",
      "189->190:C\n",
      "190->191:A\n",
      "191->192:A\n",
      "192->193:A\n"
     ]
    }
   ],
   "source": [
    "print_graph(trie[183:193])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Applying the Trie to Multiple Pattern Matching<a name = \"multiple-pattern-matching\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đưa ra một chuỗi Text và Trie (Patterns), chúng ta có thể nhanh chóng kiểm tra xem có bất kỳ chuỗi nào từ Patterns khớp với một đoạn bắt đầu từ tiền tố của Text hay không. Để làm như vậy, chúng ta bắt đầu đọc các ký tự từ đầu Text và xem chuỗi ký tự này đánh vần từ gì dọc theo đường dẫn đi xuống từ gốc của Trie. Với mỗi ký tự mới trong Text, nếu chúng ta bắt gặp kí tự này dọc theo một cạnh dẫn xuống từ nút hiện tại thì chúng ta tiếp tục dọc theo cạnh này; mặt khác, chúng ta dừng lại và kết luận rằng không có chuỗi nào trong Patterns khớp đoạn Text từ điểm đầu tiên. Cuối cùng chúng ta sẽ tìm được các đoạn text bao gồm các kí tự chạy từ gốc đến các lá của Trie, đó cũng chính là các chuỗi trong Patterns khớp với Text từ tiền tố của nó, thuật toán này được gọi là PrefixTrieMatching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/LP2D9Vy.jpg\" width=\"700\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta sẽ áp dụng thuật toán trên cho các chuỗi con của Text lần lượt từ vị trí đầu tiên đến vị trí cuối cùng của Text, thuật toán này gọi là TrieMatching. Chúng ta cần |Patterns| bước để xây dựng Trie(Patterns), mà chứa nhiều nhất |Patterns| nút, trong đó |Pattens| là tổng chiều dài của các chuỗi trong Patterns. Mỗi vòng lặp của PrefixPatternMatching chiếm nhiều nhất |LongestPattern| bước, trong đó LongestPattern là chuỗi dài nhất trong Patterns. TrieMatching gọi |Text| lần tới PrefixTrieMatching, như vậy tổng số bước bằng |Patterns| + |Text|.|LogestPattern|. Thời gian chạy này là một sự tăng tốc đáng kể so với |Text|.|Patterns| bước, khi |Patterns| lớn hơn nhiều so với |LogestPattern|. Thuật toán Aho-Corasick được phát triển năm 1975 tiếp tục giảm hơn nữa số bước sau khi xây dựng một Trie từ $\\mathcal{O}(|Text|.|LongestPattern|)$ bước tới $\\mathcal{O}(|Text|)$ bước [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining whether a symbol of text matches a edge's label of trie or not.\n",
    "def get_trie_matches_symbol(v, symbol, trie):\n",
    "    lg = []\n",
    "    node = []\n",
    "    for edge in trie:\n",
    "        \"\"\"If a symbol of text matches a edge's label of the trie,\n",
    "        we append end node of the edge to list of node.\"\"\"\n",
    "        if edge[0] == v and symbol == edge[2]:\n",
    "            lg.append(\"True\")\n",
    "            node.append(edge[1])    \n",
    "        else:\n",
    "            lg.append(\"False\")\n",
    "            node.append(0)\n",
    "            \n",
    "    \"\"\"If any label of out-going edge matches the symbol, \n",
    "    We select end node of the edge.\"\"\" \n",
    "    \n",
    "    if \"True\" in lg:\n",
    "        logic = True\n",
    "        end_node = node[lg.index(\"True\")]\n",
    "    else:\n",
    "        logic = False\n",
    "        end_node = 0\n",
    "    return (end_node, logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding patterns matching sub-string from the beginning of text.\n",
    "def get_prefix_trie_matching(text, trie, leaves):\n",
    "    # To set initial variables.\n",
    "    i = 0\n",
    "    symbol = text[i]\n",
    "    v = 0\n",
    "    \n",
    "    while 1 < 2:\n",
    "        i += 1\n",
    "        w, lg = get_trie_matches_symbol(v, symbol, trie)\n",
    "        if v in leaves:\n",
    "            return(patterns[leaves.index(v)])\n",
    "        elif lg == True: # If any label of edges matches symbol of text.\n",
    "            if i < len(text):\n",
    "                symbol = text[i]\n",
    "            v = w\n",
    "        else:\n",
    "            return(\"No matches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the first position of a text that a pattern matches to sub-string of the text.\n",
    "def get_trie_matching(text, patterns):\n",
    "    # Constructing a trie from patterns.\n",
    "    trie, leaves = construct_trie(patterns)\n",
    "    index = []\n",
    "    count = -1 # Count for index of text.\n",
    "    while text != '':\n",
    "        # To get results from matching: patterns and sub-strings of text.\n",
    "        pattern = get_prefix_trie_matching(text, trie, leaves)\n",
    "        text = text[1::] \n",
    "        count += 1\n",
    "        # if a pattern matches sub-string of text, we append index of text.\n",
    "        if pattern in patterns:\n",
    "            index.append(count)\n",
    "            \n",
    "    return(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"CACTCCTTTGCCTCAATATTCGCGGCCGTGCCCGACCGGGACGATTGGAATTCGGGCAAGGGATCGAGACGCGGAAAAATACCCAGTGCCGCGGTGCGGGGGATTGCGAGCCTAAAAGCATGCGAACTCCGCCAGGAAGATTGCAAACCAGTTCCGTGTGATACTTCCTACTTCCTACGCATTTACTTGCGTTTCGGCTGTAAACTAGCGCCATGCCGTCGGGTACTATTAGACGCCGAACTACTCAGTACAATTAATGTCTGGCTGTGGAAAGCACTCCCTGCCACCATGACTCGATAATTAACGTATTGGTGCGAGATTAACGCGCTAGCGTGGGCAAGGGGCAAGGGTTTACAGAGTTGGCAGTATTTGCGCCTAACTTCAAAAACATTCGCCATCCCCAACGCTACGCCCAGGTCATAAACTAGCTAGTTGGCACTGCCATATACTCGCGGGTCGCGGGTCACACATATCGCGGCGAATGCCGCCATAAAGTTACAATATGTAGTCTTCGAGCGTTTGATCTTTGATCTTGAGAGCCTAATATGGAATGGGTTGGTCGGCCGCGGTACAGGTGATCAAACGAGAATAATATTGTCCACCTCACGAAACTTCATTTCATTCCAGCCCGTGTTCCCGCTGCCGCTTGAATGTCTAATTAGTGAATGGGCATATCTCATAACCTCTGCCTTTTGCTTATGTAATAGTTCAGCTCGCTTCATGTAAAGCAAAACTTTCCAGTTAATCATGAGTCCATCACACCGCCAAGATTTGAAAACTGGGGGCACTTATATGTACATAAGTGCCCATCTTGCCATATGTCGCCTTACTAGGATCCGCCGACACATTCTCCGGACTGGTGGAACTCGGGCCCAGGCTAGCTGATTATACCAAGTCTTGGGGTGCGTGCTCCCCTGTACGCTTTTCAGTCGGCCGTTTTCTCGCGGGTCATGGTCACTAACAGGACCCATAACCTCTATATCCGACTAGAGAGTTTAAAACTAGCAAGGGATTCTCTGTGTCTATCCTACGTAACTCCCTGACTTCTAGACACGGTACGATGCTGTTGCAAGAACTGTCACTACTCTACTTCCTACGCGGAACACGTATATAGCATCTGGTTGACGCGACTTTAGGAGAACAGACCCCATTCCTGTCTTATACGCCCATAACTAGGGATCCCTGAAAACGCAGTCCCCAGCAGGTCTTAGCAGGATATTTTTGCGAAAGAAATGGGTGTCGCGATTAGTACTCTGGGCTTGGTCTATACTGGCCATTATGCAACACTACACCATCGGAATGGTCCTCCCAGTCAGGGGGCTCCCGGCGACTCGCCCGTTTGCGATGCAGTCGGAGTCGATGGCAGTTCTTCGCAGGCCAGTTTACGAGAACAGACTTACAGTGGATCTTTAATCGTCAAGCGTTCGAGTGTCAGCAAAATATCATAGACTGACCTCCAGTCAGCGGCCTTAGGCCATATCCCGGGATGGTTTCGTTCGCTGGAATCCACTGTCTCCAGTTCCGTCCGAGGCCGCCACCGATAAGTCCGCAGTTCTCCACAGATTTGGTCCCATGGCGTAGTGTTGTGCCCCGGCGAGCTGTACTGGTTCGCGTCAAATGTTACAAACCAAACTTCAGATTAACGACGTCACTCTATTTAATAGGCGCTGTAGACCGTGTTTTAGCAAAAACAATTCATACAAAAGCGCGGAGCGCGCCATTCGAAAGCACTGCAGAATGAAAGGCTGACCTGATCCCTATGCCCGCCGTATACGAGTACGACATGTACAACACGTGCCCTTTTTCCGAATGACCGTAAGGGAATGCAACATGGCCGCTTAACCTGAGGAGGACACAGCCTATTAGATAGCGCGTGTATACCCGATAAGAATCCATGGCAGAATCGGATAGTAGAGGGATACTGCGCGTGAATCGCAGGTAGCCAAATCCGCCCGACGGAATGAAAAGAGGAAAGCGAAGGTCGTAAGCTTCGTTGCGGTCGCAGGCTCTACATAAAATGACGCCTACATTCCGTAGGAACGCTCTAAATTAAGCGAAGTTTTACTTTTGTGTTGGGCGTAGCAACTGTCATTAGCTTCCAATAGATGTCGGCTACCTCTTTAGAGGTAGAATCAGACGAGTTCTCGCTAAGTTGTAGATCGGATCCAGCGATGACACACCTTTTTGGCCGACTCGCCCACAATACGCTCCGGACTCCGGACTTATGTTACTTTCGAATCCTGCCTTATTGATATACAGGCTCGGTGATTCATACTGCGTACAGCGTCTATGCTAGTCGAAGAATCACTGCCACCAGGCCACAAATGCCACGCGCGACACTTGAATCAGCTGCGTGTCGAGCATTGATTAACAACTAAGGATGCGGCGGCATATGTATAATTGGTCTACGTACCCCCTTACAGACGAACCTCGCCCCAGCCCATTAACTTTACGTGGTACTTTTAAAAGAAGATTCCCTCATTGCAACTCGACCGCTTCGGCTGACTAACAGCAGATATATGAAAGTGCTGGGCACCAATGGAATTGCGTAGAAGCAAGCCACTTTTTGGCGACAATTACGTCCCACGGTAACGGGCGGACCGCATTCGCGCGGTCCAATTATATCTGACATGTTTATTCATAATCCTAACATCTCCTATTTCCATTTCAGGCGACAAGAGCGGGTCCTAGCTTGAAGGAAGCCGCAGTATGTAGTAGAGAAATGGTGAACTTTCAGTAACGAGTCAGTCACGAGAGGTCGTCCAGGGCTTGGGTGCGGTCTCAATTCACCTTAAGATGACTGGTCTATGTTCAGCTCAGTAGTCCGCTCCTCTTTGTTTGTTTAACCCCTGACCGGATTGGTAGTCACCCTGGTTCCTAATACGACCCAGACGGGCTGGCTTCGATCCTTGCGATCCGAAGTCCTATTTCGGGCCCTGGCGCTGAGCCAAGAGTCGCTCGTCGGCTTGAAGGCGCCAGCAGATTTCGGCCTTCTGGGTGAGGTGCGATAATTGCATCATTTAGAACATAGCCTACTTGATCTGCTCACCCCGGTACGGCCTAAGACCTGCTAAGCCCGCGTTCGCCTACAGATTGGCCGCTTCAGCATTACCAGGCGGGCAAGGGGCAAGGGAGTCTTCATAAAAATGCCAGCTAGGTGCACTTCCCGATAAGCGAGAACAGAGAACAGATACGCCGCTCGTCTTGCTCCTGCAGAACCCTGGAAACGGTGCTTGTATCAGCTCCCCTCGGACCTAGGTCGTGTATTGGCTAGGCTTGACGGGTATTTCGATCTATGTCCCTAACGGCCACCATTCTAGGACGGGGTAATAATACCCGTGAGAACCATTTTACTGCGTCTAGGGTGCGTCCACTCGCTCAATAGTGCAAGTTCTTGGTTGCACCTTTTCTCCGGACTTGCTCTTGTACAGATCCCTACCCACGTCAGGAGAACAGAATGGAAGTACTTAGAGAGATCTACTGTTGCCTGGTTACCTAAATCACAACATATCACTTGGTGATAATGGCCCCCGTTGTACCAGGGCGGTTACATAATAAGTCCATGTCTAAGTTAGCGTGCAGTACCCAGCAAATCGAGGAGCATTATCGGTACGTTCGATTATGTGATGCTTCTGATTGGGTGCCCCATCACGCGCAGCCCATACGACCAGAATGAACGCCCTTACGCTGTTATAATGTCTACTAGCCCGCCCTACATAATTATATCTATCGGAGTGCGCTCACCACTTAGTCCGATAGCACGCAGGATCGCCTACTCAGTAAGGATCAAGACATAACTCTACCCCTCGCGGGTCGCGGGTCCGTACGACTACCGACTGCTGGGGCAAGGGGCAAGGGTATCAACTGGGTGTGCCGGGATGGTTAAAAGTCCATCGGAGCCTAAGCTATACGCCATGTCCAGCCATTGAAGTTAGGCTTCTCTCCCGATAAAAAAGTTCGTCAAATGGTATTTCCACGGAATCACGGCCTAGATTCAGTTATGGATTGAGGATTTAAAGAGAGCGTAATGCAGATAGTCGCGGTGCGTTATGTCGCGAGGAGCGACACTTTGCCCGACGACCCTGCTAGTATTCCGCGCATATGAGGTAGTCTCCTAAATATCCTGTCTGGCTTGTGCTGGATGAGGCGAAAGAACTGTAACGCGCCCGTGGGGCATGTGATGTCCTCGGGTACTTAGATGGTCTTGGTCCTAGGGCGGCGTTCTCAGCCGCTCATTGGGTTTACAACTTTGATCTTTGATCTTCCCAGGTGCGAGCAACTCCATGCGGTGATGGAGTCGTGGGGTTGTTTCCTGTTGATGGTGGCCGACCGAGGAATGCCCAATAAGTCGATCTCGCTCGTATCCCGATCCCAGGATGGAGATTACCCTGTGCGAAGAGGTCGGTAGGTCAAAGCTCTTAACATGGTTTTTCCCTTATGCCTCGCAATTGAAAACCGGAACACTCATGACCGTGGTCGATCATATGAAATAAAACCCCCGGCAGAGTATGAATATGGGCAAGGGCCCTCAAGCCCCGAGGCGCAGACCGCGGGAGACTATCTGCACTAACCTTCGGTCCCACCTCTATTAGGGCGCGCTCATACTGCGTTTGGACTGAGACCGATAGAATACTACTTTGATCTTCAGTACGATCGCATCATGGTGCCTTACCAATGATCCACTAGTGCGCAAAATAACCGGTTATAACCAAACTGAACCCGATTACTTAAAAGGCCGAAAGTACTTCCTACTTCCTAGAGAAAACGAGTGTTCACGACGATATTGGTCCAATTGAGATGCTTTTTTTTCACAAGGGGATTTGCCTGGAGACACACGGATCTCGGATGGGGCGGACATAGCTTACTACACCTGTTAGACACTGATAAGTTAGGGTGGATAATGCACCGTAGGCAAGGAGTACGAGCTTAGAAGTCTGAGCGCGAGAACAGAGTTTGATCTTTGATCTTCAATCTTGACAGTATTAGGGCTCACTCCGATGGTTAGTTCGGTAAAGTGACAAGAGATGTTTCTATAGGTGCAGTTGCATCTCACACGGTAACAACGGCTGACACGGCGGAAATGCGGTGAACTAACACGACTGTCACTCTGGGGTTGCGGCTACACAACAGATGTAAAGTCAAGTCAGCCATGGGAGGATACCCGGGCTACCCTATCTCTCAGTGGGGCCAATCGGTCTTCGGTGCAAGTATGCATCTTAACCTCTGTAAGACGTAACCACAACCTCCTCATGGTACTACACAGTACATGCGAGTTAGTTTCGGTATTCGAGCGCGTGACTGTATATGACCATGATGCTGTGTGAGGTTTCCTGTCGAACGAGTGCTATCCCAATCTTTATAAGCGTAGGAGGTCCCTAAAGCGGGTGTTTGCATGAAGTCCTGTTGTGTTTCGAATAGAGTATTCATAGAAACGAAACGTTAACCCATCGTGTGAAAAGAGCGGTCGATGTTGACGAGAGTGGAAGATCGTGTCCCTGTGAGGGCTACTTCCTACTTCCTACGATTGCCTTTCTATCCTTACATGCTAACATCTAACATCTGCTGCTTCTCTGACCTCACGCGGCCAGCTTAAATGTTCCCTCAAATTCTGACTACACCAACGCACATCTTCTGACGGGCATGACAGGCGCAATTGGCGATCACGAAAGCGAAGGGTGTCCCGTGTGGCCGTTAGCGTCGCTCGCGGGTCGCGGGTCACCGGCTACGCACTTTAGCAGCGAAATAGAGTCACATCGTCGTTGGGAGTCATGGTATCCATAACGCCAGCTTGTTAGCACGACCTTTAGCTTACTCATAGTCTCGACCTAACTATTATTTTGATCTTTGATCTTGCACTTAGCTAACATCTGATTTCCGTGTGCCATAGGGGTAGTTACTTTCTCACCCACTGTGAAAAGTATTGCGATGAAGGCAACACTAGTTAACGGAATTCCTAATGAACATCATAGAACGAGAAATAAAGCAAAATCTGAACATTCTGAGGGCCGGATACCCTGACACCAAAATCTTTAAGCACGACCGGGCCAAATGTACCCCTTAATGCGATGTCTTCTATATTGATAAGGTATTGAACCTAATAGGCCCGAGAATCACATGCGTTATTACGCCACTCCTGATGATGCCCTTTTGGCCATAAAGTAGGGTAAACGACGAACACAAAGACGGGGCCCATGGAAATGATGTTTGTTCCCAGATCGTGCTTTAAGTATGCTCAAAGACCAATGTGGGCTATGTCAACGGGGGAGAATTACGCTAGCGTCCTATTATCGGCAACCCTCCTTGTATCCAGAGGTCCATACAAAGTCGTGCTTGCCTATCTCACTTTTTTACGGGAAGCCATCAATGTACCTGACTGGGGCGGCGTAAAACAGCGGGACGGACGTGCTCTTTCGTCCCTGTAGGCGCCTGCTATACTGGCAGGCTCACTGTTTCGCTGTCCCTATAGCCGTGGTTGTTACATTCCGATACGACCCGGGTGGCCGTCGTGCATAATCGGTCCATCCGCTAAGTAGGACGGAGTCGCGGATCTTCCATCATAAAAACCTCCTGCCTTAGGAAGCACAGGTTTAGAAGACAATCGTTTAAAGCTGCAGCTGGATAGGCAGGCAGAACTCCTGAAGTGGCACATCTTAAAGACACGAATAACACTCCGGACTCCGGACTGAACATCTTGTTACTTGCCCGCACAATTATAACTGACAACCTGACTCCGCCGCCGAATCCACATAGTGGGAGAGTACCTCGAGTTACCAACGGAAGGCATTTCATACTTCCTCCAAGGATAGCAAGGAAAGTCAGGGTCCCGACTAGGGGACCTTGGTTAGAATGTCCGGATGATTAATGCGCATCCTTCTACTTCCTAAGGGGTACGTAGGTTCGTTCACGGATGGAAATTTCGCGGGACGAAGTGACTTACGCCCGCTCTTCGAAACAGCCGACGTCTGGCTTAGGCCCGCCGGCATTGTTCGGAACAGAGACGAGTTCGATGAGCATGGTTCTTCATGGTCACGCAGGGTCAGGTATCAGACGTCCACCCATTACCTTGACCTTCTCATTACGCCGCTTGAATGGCCAAGCTCATCTTCGAATAAGGTGTGCACGCTAATGTCGTAACATTGCCTCTGAATAGCCTATCAGCAAGTTTCCGTCCGGCGCAGTCGGTTAGAGTGCGGAAATTTCAAACATGTAATCTGACTTACTAATTAGGGCACCCGATGATCTCTGTGGGCATCATATCTCTTCTGGTGCCGCTCGCATGTCAGTTACTCGGATACTCTAGTAACGACGCGTGTACGGGGGCGGAATGAATTGATGATGTTTTCTGCAACATTTAGCGCGACATAGCAATTGCTTGGAAGGTCATGTCCATCGGCCTATCGACCGACTGCCTTCATGACTCCTCTCGCGCCATCCGTTTGATGGCTGTGAATGGTTCCACTATCAGGTGATAGAGGACAACGTACCTCATTCACGCCTCCAAGTGGCCTGAGTATGCGCGGCTTAATCCCTACGACATGCCATGTTAGAACTTGTCGTACCACTGTCGTATCGCGAGCTCTTGTCCTGTGGCTATTTCACTGCGCAGAAACGGTCCAATCTACGGCAATGCCGTTATACACGAGGCACGAAGTCGAGTGCGTGTCTAATGAATAGGTGAGTTCAAGGATAGGCGTGGAGGTCGTAAAGCAGCCAATGGGGCCTACAACACAAAGAAGATCTATAAATCGTGTGAGAACAGAGGGTACGAACGACGAAAAAAGACCAAAGGGGTTATGTGGGAGATAAGTCATTCCAGAGCAGCGTCACTTACGCAAAGCAAAAAGCAAAAGGATACTTGGTGCATGCCGTTAGTCTAAGTCGACATCAGAAACCAGTCCCAAATACACGTCGATGAGCGTGGCGAGAGGCGGCATCTCACTTACAAGTAGATAACTAACATCTAACATCTCCCGGCTGTCACTCGCTACAACCTGGGAGCTGCTCAGAGCAATAAAATGTCTTGCTATAAAAAACCGTTTGGGGCCAGGTAAGGACATTAAGGAACGCCTAAAAGATGCGCGACGCACTGACAGCTTCTCAATGTAGTCACGCTTCGAGCTCGGGCTGTGCCCACTTATGATACACTTCGAAACTCACACTGCCGGGTATATATGCACTTGGATTAAGCGCGACTGGCACAGACTACTGCGCTCAGGCGGTTAGTGCAGCGCGGCCATGGGCGTTGCCTTGACGTATCGCTCCAATCGCTGAACAAGTAAGCGCATCGGTAATGGCGAGTTGGAGAGCCCACAGTGCAGGGGGGCTCGCGGGTCGCGGGTCGCGACCCCACCAATGGGCGTGCGTTCGGACACCACCGCATTATATTCACAAAGATGCTCAATGAATAATCTGGATCCTATCCTCCTTTGAGACCAGGGGCTGTGGCGGACCTGCAATTAGAGATACAAGGAAACGCACTCCCACCGTTCTGGGCTGAAAAAGAGTTGTGATTCTAGGATGGGAACTAATAACCCGGCTGAGGCGAT\"\n",
    "patterns = ['GGGCAAGGG', 'TACTTCCTA', 'AAAGCAAAA', 'CTCCGGACT', 'GAGAACAGA', 'TCGCGGGTC', 'TTTGATCTT', 'CTAACATCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 161, 168, 334, 341, 449, 456, 518, 525, 724, 849, 941, 1087, 1136, 1386, 2207, 2214, 2646, 3117, 3124, 3175, 3182, 3409, 3448, 3806, 3813, 3842, 3849, 4248, 4255, 4516, 4636, 4742, 4749, 4942, 4952, 4959, 5505, 5512, 5545, 5552, 5701, 5708, 5836, 5843, 5860, 5979, 6668, 6675, 6874, 7751, 7833, 7840, 7953, 7960, 8324, 8331]\n"
     ]
    }
   ],
   "source": [
    "index = get_trie_matching(text, patterns)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù TrieMatching nhanh nhưng việc lưu trữ một Trie sẽ tiêu tốn rất nhiều bộ nhớ. BruteForcePatternMatching làm việc với single read tại một thời điểm với bộ nhớ thấp vì chỉ cần lưu trữ bộ gen trong bộ nhớ, trong khi TrieMatching cần lưu trữ toàn bộ Trie trong bộ nhớ mà tỉ lệ với |Patterns|. Vì một tập hợp các lần đọc cho bộ gen của con người có thể tiêu thụ trên 1 TB nên bộ nhớ cần thiết để lưu trữ Trie là rất lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Suffix Tries <a name = \"suffix-tries\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì việc lưu trữ Trie (Patterns) đòi hỏi rất nhiều bộ nhớ, chúng ta hãy thay thế Text thành cấu trúc dữ liệu. Mục tiêu là so sánh từng chuỗi trong Patterns với Text mà không cần duyệt Text từ đầu đến cuối. Nói một cách quen thuộc hơn, thay vì đóng gói các Pattern lên xe buýt và đi đường dài xuống Text, cấu trúc dữ liệu mới sẽ có thể dịch chuyển tức thời mỗi chuỗi trong Patterns tới vị trí của nó trong Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một <b>Suffix Trie</b>, ký hiệu là SuffixTrie(Text), là một Trie được hình thành từ tất cả các hậu tố của Text. Từ giờ trở đi, chúng ta sẽ thêm ký hiệu đô la (\"$\") vào Text để đánh dấu điểm cuối của Text. Chúng ta cũng sẽ dán nhãn cho mỗi lá của Trie theo vị trí bắt đầu của hậu tố có đường đi qua Trie kết thúc tại lá này (sử dụng lập chỉ mục dựa trên 0). Theo cách này, khi chúng ta đến một lá, chúng ta sẽ biết ngay hậu tố này đến từ đâu trong Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/ZY4UQHc.jpg\" width=\"600\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns_from_text(text):\n",
    "    \n",
    "    patterns = []\n",
    "    for i in range(len(text)):\n",
    "        patterns.append(text[i::])\n",
    "        \n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a suffix trie from a text.\n",
    "def construct_suffix_trie(text):\n",
    "\n",
    "    trie = []\n",
    "    leaves = []\n",
    "    new_node = 0\n",
    "    patterns, prefix = get_patterns_from_text(text)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        current_node = 0\n",
    "        concat_symbol = ''\n",
    "\n",
    "        for i in range(len(pattern)):\n",
    "            current_symbol = pattern[i] # Get current symbol.\n",
    "            all_out_edge = get_out_edge(current_node, trie) # Get all out-going edges.\n",
    "            # Get end nodes of out-going edges and corresponing labels\n",
    "            end_node_label = get_end_node_label(current_node, all_out_edge) \n",
    "            # Finding end node that its correspoding label and current symbol are the same.\n",
    "            lg, node = get_out_going_node(end_node_label, current_node, current_symbol)\n",
    "            # If logic =  True, only setting current node to end of out-going edge.\n",
    "            if lg == True: \n",
    "                current_node = node\n",
    "            # If logic = False, we add new edge to the trie and set current node to new node.\n",
    "            elif lg == False:\n",
    "                new_node += 1\n",
    "                trie.append([current_node, new_node, current_symbol])                 \n",
    "                current_node = new_node\n",
    "        # Finfing leaves of the trie.\n",
    "        leaves.append(current_node)\n",
    "\n",
    "    return(trie, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng nếu một chuỗi đơn Pattern khớp với một chuỗi con của Text bắt đầu ở vị trí i, thì Pattern cũng phải xuất hiện ở đầu hậu tố của Text bắt đầu từ vị trí i. Do đó, chúng ta có thể xác định xem Pattern có xuất hiện trong SuffixTrie (Text) hay không bằng cách đánh vần từ gốc (root) đi xuống, nếu điểm cuối của Pattern vẫn nằm trên đường dẫn xuống một lá nào đó ta sẽ nhận được một Pattern khớp với Text bắt đầu từ hậu tố của nó. Quá trình được lặp lại cho tất cả các Pattern nằm trong Patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ: Tất cả các đường dẫn bắt đầu bằng \"ana\" cho thấy ba lần xuất hiện của \"ana\" trong \"panamabanana $\". Mở rộng các đường dẫn này đến các lá (hiển thị bằng màu xanh lá cây) cho thấy rằng vị trí bắt đầu của những lần xuất hiện này là 1, 7 và 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/VKu2lz4.jpg\" width=\"600\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy nhớ lại rằng việc xây dựng Trie (Patterns) cần có thời gian chạy và bộ nhớ $\\mathcal{O}(|Patterns|)$.\n",
    "Theo đó, thời gian chạy và bộ nhớ cần thiết để xây dựng SuffixTrie (Text) bằng với tổng độ dài của tất cả các hậu tố trong Text. Có |Text| hậu tố của văn bản có độ dài từ 1 đến |Text| và có tổng chiều dài |Text| · (|Text| + 1)/2, như vậy runtime sẽ là $\\mathcal{O}(|Text|^2)$. Vì vậy, chúng ta cần giảm cả thời gian xây dựng và yêu cầu bộ nhớ của SuffixTrie để thuật toán trở nên khả thi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Suffix Trees <a name = 'suffix-tree'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể giảm số cạnh trong suffix trie bằng cách kết hợp các cạnh trên bất kỳ đường dẫn không phân nhánh nào thành một cạnh duy nhất. Sau đó, dán nhãn cạnh này bằng hợp nhất của các kí tự trên các cạnh. Kết quả của cấu trúc dữ liệu được gọi là cây hậu tố, được viết SuffixTree (Text) [13]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để khớp một Pattern với Text, chúng ta sẽ xâu Pattern vào SuffixTree (Text) theo quá trình giống như được sử dụng cho suffix trie. Tương tự như suffix trie, chúng ta có thể sử dụng nhãn lá để tìm vị trí bắt đầu của các Pattern khớp với Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/wBv2OQp.jpg\" width=\"600\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So với suffix trie, số lượng nút có thể lên tới bậc hai của chiều dài Text, số lượng nút trong SuffixTree (Text) nhiều nhất là 2·|Text|. Do đó, bộ nhớ cần thiết cho SuffixTree (Text) là $\\mathcal{O}(|Text|)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffix tree tiết kiệm bộ nhớ vì chúng không cần lưu trữ các nhãn của các cạnh được nối từ mỗi đường không phân nhánh. Ví dụ, suffix tree không cần 10 byte để lưu trữ các cạnh lần lượt có nhãn \"mabananas\"; thay vào đó, nó chỉ lưu trữ một con trỏ đến vị trí 4 của \"panamabananas\", cũng như độ dài của \"mabananas\". Hơn nữa, suffix tree có thể được xây dựng trong thời gian tuyến tính mà không cần phải xây dựng trước suffix trie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù suffix tree giảm yêu cầu bộ nhớ từ $\\mathcal{O}(|Text|^2)$ xuống $\\mathcal{O}(|Text|)$, nhưng về trung bình nó vẫn cần bộ nhớ gấp 20 lần so với Text. Trong trường hợp của một Bộ gen người 3 GB, bộ nhớ RAM sẽ là 60 GB, đây là một cải tiến rất lớn so với 1 TB mà chúng ta cần làm việc với Trie (Patterns), nhưng nó vẫn đưa ra một thách thức về bộ nhớ. Điều này cho thấy một bí mật đen tối của ký hiệu big-O, đó là nó bỏ qua các hệ số không đổi. Đối với các chuỗi dài như bộ gen người, chúng ta sẽ cần chú ý đến hệ số không đổi này, vì biểu thức $\\mathcal{O}(|Text|)$ có thể áp dụng cho cả thuật toán với\n",
    "2·|Text| bộ nhớ và thuật toán với 1000·|Text| bộ nhớ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần code thuật toán dưới đây chỉ mô phỏng lại một suffix tree chứ chưa áp dụng thuật toán cải thiện đáng kể khối lượng lưu trữ như đã nêu ở trên vì nó khá phức tạp, chúng ta sẽ trình bày code này ở một phần khác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a suffix tree from a text.\n",
    "def construct_suffix_tree(text):\n",
    "    tree = []\n",
    "    # We still need to construct a suffix trie.\n",
    "    trie, prefix = construct_suffix_trie(text)\n",
    "    # Transform a suffix trie to a suffix tree.\n",
    "    for edge in trie:\n",
    "        all_out_edge = get_out_edge(edge[1], trie)\n",
    "        # Concatenate all edges of non branching path to only edge.\n",
    "        if len(all_out_edge) != 1:\n",
    "            index = trie.index(edge)\n",
    "            concat_symbol = ''\n",
    "            for i in range(index + 1):\n",
    "                concat_symbol = ''.join((concat_symbol, trie[i][2]))\n",
    "            tree.append(concat_symbol)\n",
    "            trie = trie[(index + 1)::]\n",
    "                \n",
    "    return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ATCTACCAGCAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'T',\n",
       " 'CTACCAGCAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'C',\n",
       " 'TACCAGCAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'C',\n",
       " 'T',\n",
       " 'ACCAGCAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'AGCAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'C',\n",
       " 'C',\n",
       " 'AG',\n",
       " 'CAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'C',\n",
       " 'AG',\n",
       " 'CAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'G',\n",
       " 'CAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'CAGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'C',\n",
       " 'AGTGAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'GAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'GAACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'G',\n",
       " 'AACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'AACATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'A',\n",
       " 'CATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'C',\n",
       " 'ATGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'TGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TGGGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'GGAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'GAGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'G',\n",
       " 'AGGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'GGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GGACCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'A',\n",
       " 'CCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CCAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'C',\n",
       " 'CAGTAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TAAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AAGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'AGGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'A',\n",
       " 'GGAAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GG',\n",
       " 'AAGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GGCTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTTACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'TACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TACCCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'AC',\n",
       " 'CCTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CTCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TCGATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CG',\n",
       " 'ATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'ATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'G',\n",
       " 'ATGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TGTGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TGTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'GTTACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'TACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'ACAGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'ACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'ACTCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TCGTTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'TTCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'TCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'T',\n",
       " 'TCGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'CGTAGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'AGGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GGGTGTATAACGCCGCCGCTGG$',\n",
       " 'GTGTATAACGCCGCCGCTGG$',\n",
       " 'TGTATAACGCCGCCGCTGG$',\n",
       " 'TGTATAACGCCGCCGCTGG$',\n",
       " 'ATAACGCCGCCGCTGG$',\n",
       " 'ATAACGCCGCCGCTGG$',\n",
       " 'TAACGCCGCCGCTGG$',\n",
       " 'TAACGCCGCCGCTGG$',\n",
       " 'AACGCCGCCGCTGG$',\n",
       " 'CGCCGCCGCTGG$',\n",
       " 'GCCGCCGCTGG$',\n",
       " 'GCCGCCGCTGG$',\n",
       " 'C',\n",
       " 'CGC',\n",
       " 'CGCTGG$',\n",
       " 'CGC',\n",
       " 'CGCTGG$',\n",
       " 'GC',\n",
       " 'CGCTGG$',\n",
       " 'TGG$',\n",
       " 'TGG$',\n",
       " 'TGG$',\n",
       " 'TGG$',\n",
       " 'GG$',\n",
       " 'GG$',\n",
       " '$',\n",
       " '$',\n",
       " '$',\n",
       " '$']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_tree = construct_suffix_tree(text)\n",
    "suffix_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Suffix Arrays<a name = 'suffix-arrays'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Năm 1993, Udi Manber và Gene Myers đã giới thiệu suffix arrays như là một sự thay thế hiệu quả về bộ nhớ cho suffix trees [14]. Để xây dựng SuffixArray (Text), trước tiên chúng ta cần sắp xếp tất cả các hậu tố của Text theo thứ tự alphabet, giả sử rằng \"$\" đứng đầu trong bảng chữ cái. Suffix arrays là danh sách các vị trí bắt đầu của các hậu tố được sắp xếp này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/eXODwwe.jpg\" width=\"400\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài toán xây dựng Suffix Array có thể dễ dàng được giải quyết sau khi sắp xếp tất cả các hậu tố của Text, nhưng kể cả các thuật toán nhanh nhất để sắp xếp một mảng gồm n phần tử cũng yêu cầu $\\mathcal{O} (n · log n)$ sự so sánh, sắp xếp tất cả các hậu tố cần $\\mathcal{O} (| Text | · log (| Text |))$ so sánh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên, tồn tại một thuật toán nhanh hơn để xây dựng các Suffix Arrays trong thời gian tuyến tính và chỉ cần khoảng 1/5 bộ nhớ so với Suffix Tree, có thể giảm yêu cầu bộ nhớ 60 GB  cho bộ gen người xuống tới 12 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_suffix_arrays(text):\n",
    "    patterns = []\n",
    "    for i in range(len(text)):\n",
    "        patterns.append(text[i::])\n",
    "   \n",
    "    suffix_array = np.argsort(patterns)\n",
    "    patterns = sorted(patterns)\n",
    "        \n",
    "    return(suffix_array, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'GCCAGCCACACGCTAACGACACCCCGTCCACGATGGTAGTTTAGGTCATTCCAGGGCCACTTGAACCTTAGTCAGCTTGACTAGGTAAGGTATCTCCTTGTGCCGGGAAAGGTATCGTGCATAGGCTTAAGAAGACAAGCCGTTAGTTATCCTGTATTGCTCCGCCCGTTCAAAGCATCGGCTCGGAGGCCCAGAGTGGAGCGCATGTCTCGGAGACTTATCTGGCCACATTGATTGAGGTAAGCGAATCATCGATGGGTAGACTAAATGTAATTTTCTCGTCTATCTGGTCAAATGCCGCCAAGCAAGGTTAAAATTAGAGGCGCGCCACGGCTACAGTCTCGACCCATCCTCAATTATATCTGGGTTATACCCGGGACCACGCCGTCCTAACACCGGGTCATAACCGGAAACCAACGACAGTCACTTATGATCTACTGCATCGAGTGTCTCTCCGCACCCTCATCATCAACGCACTTGATAGGTGGATAGCATATGCTACGCATCCGGTACAGGCGCGGCACCGTCCAGGACAGTTATCGGATCCTGTGGCCGTCTAAGACCATCTCTGCTCGAGAGAGAGACAACAAGTTTCCTAAGCCTAGGCGCGCCCGCCGTAAGAGACACGCCGTCCGTCACTCTACCGCCGCGAGGATTTGGAGTAATCTCGTAGTGAGCGATCATCAACTTGGAGGCGGATATCCTCACTCGTCATGACAGGCATTGGTGTTTCTATTGAGGGCTCACTTATGATCAGTCGGCCGCAGTCTCGTGGTTTATCCCTTGGTCAACGCCCTGGCCCCCTTTCCTAAGCGTGTAGTACTTTCATTTATCGTAAGAGTACTTACTGGGCTACAAGGGGGGTGCCAGTTAGTCCTTTGCCCCGGACCCATTATAGCTATTCTCATATCGGCTAATTAATAACCACCTCCCCTATC$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[938, 312, 410, 171, 107, 292, 265, 313, 585, 391, 411, 922, 404, 63, 14, 415, 470, 789, 685, 128, 131, 558, 618, 836, 302, 172, 136, 597, 241, 810, 856, 108, 86, 306, 588, 919, 246, 663, 293, 266, 915, 314, 354, 271, 583, 134, 854, 586, 18, 392, 623, 7, 716, 511, 419, 335, 532, 227, 412, 923, 378, 561, 344, 887, 20, 371, 458, 642, 405, 394, 522, 926, 64, 15, 416, 29, 471, 500, 790, 625, 381, 9, 329, 262, 79, 706, 637, 436, 846, 842, 215, 745, 425, 58, 475, 686, 821, 129, 581, 132, 621, 559, 260, 213, 579, 619, 577, 575, 318, 837, 192, 303, 490, 173, 3, 137, 598, 242, 675, 199, 811, 896, 73, 529, 651, 718, 186, 320, 603, 513, 692, 122, 52, 738, 857, 237, 82, 109, 87, 42, 482, 307, 660, 839, 818, 421, 69, 872, 755, 337, 765, 671, 194, 445, 868, 144, 534, 37, 589, 920, 402, 369, 488, 894, 120, 480, 698, 906, 358, 493, 935, 467, 682, 752, 464, 679, 247, 778, 504, 348, 700, 148, 543, 441, 250, 538, 908, 176, 831, 113, 432, 91, 664, 564, 219, 360, 284, 713, 749, 429, 294, 495, 254, 32, 267, 204, 916, 315, 891, 355, 47, 900, 734, 233, 229, 155, 722, 827, 654, 272, 937, 170, 291, 584, 414, 469, 788, 684, 301, 135, 855, 305, 587, 353, 6, 226, 19, 457, 393, 521, 925, 28, 624, 380, 8, 328, 705, 636, 744, 424, 57, 474, 191, 2, 72, 528, 717, 512, 51, 420, 754, 336, 764, 867, 533, 401, 119, 905, 492, 466, 681, 463, 503, 347, 440, 249, 175, 563, 712, 203, 890, 46, 228, 721, 826, 413, 300, 5, 225, 924, 27, 379, 327, 56, 190, 1, 527, 50, 866, 346, 562, 889, 189, 345, 888, 799, 881, 21, 930, 800, 610, 882, 372, 22, 164, 931, 459, 793, 780, 801, 454, 761, 297, 161, 643, 611, 646, 406, 883, 102, 373, 395, 506, 614, 632, 23, 523, 628, 384, 552, 139, 165, 388, 594, 807, 600, 932, 350, 702, 460, 927, 794, 150, 545, 65, 781, 95, 802, 875, 244, 16, 417, 342, 573, 649, 443, 677, 252, 30, 455, 472, 762, 501, 201, 298, 325, 608, 162, 791, 644, 612, 626, 382, 647, 323, 606, 516, 10, 407, 884, 210, 183, 695, 540, 518, 758, 910, 330, 178, 103, 374, 396, 507, 615, 833, 668, 633, 709, 24, 524, 629, 385, 553, 279, 115, 770, 813, 140, 166, 263, 389, 12, 556, 595, 808, 913, 852, 333, 640, 498, 434, 601, 80, 933, 282, 898, 732, 351, 703, 742, 903, 461, 928, 452, 159, 93, 340, 571, 208, 181, 666, 707, 277, 768, 638, 450, 566, 437, 568, 221, 795, 847, 362, 286, 151, 546, 125, 843, 66, 216, 746, 426, 59, 75, 476, 687, 782, 96, 822, 803, 876, 409, 106, 62, 130, 245, 582, 133, 17, 622, 715, 418, 531, 377, 560, 343, 886, 261, 78, 214, 580, 620, 212, 578, 576, 574, 674, 198, 650, 185, 319, 691, 737, 236, 659, 838, 193, 444, 487, 479, 697, 751, 678, 542, 431, 253, 31, 232, 653, 304, 456, 520, 473, 763, 118, 491, 502, 439, 174, 202, 720, 299, 4, 224, 326, 55, 0, 865, 188, 798, 880, 609, 163, 792, 760, 296, 645, 101, 613, 627, 383, 551, 138, 599, 243, 648, 676, 200, 324, 607, 322, 605, 515, 694, 517, 812, 11, 912, 851, 332, 497, 897, 741, 158, 570, 180, 124, 74, 408, 105, 530, 376, 885, 211, 197, 184, 690, 658, 486, 696, 541, 652, 519, 719, 223, 54, 187, 797, 759, 550, 321, 604, 514, 693, 911, 850, 331, 740, 179, 123, 104, 375, 53, 849, 739, 858, 859, 860, 256, 397, 861, 364, 238, 83, 508, 257, 34, 110, 88, 288, 785, 398, 43, 862, 483, 725, 308, 365, 773, 616, 834, 239, 84, 661, 269, 509, 840, 819, 258, 816, 669, 35, 111, 89, 153, 289, 786, 634, 422, 70, 399, 710, 44, 25, 525, 630, 386, 873, 756, 554, 280, 338, 206, 766, 448, 672, 116, 863, 99, 195, 484, 548, 771, 814, 446, 726, 309, 869, 141, 366, 145, 535, 167, 38, 774, 590, 728, 311, 264, 390, 921, 403, 13, 127, 557, 617, 835, 596, 240, 809, 85, 918, 662, 914, 270, 853, 510, 334, 370, 641, 499, 435, 845, 841, 820, 259, 317, 489, 895, 602, 121, 81, 41, 481, 817, 68, 871, 670, 143, 36, 368, 893, 357, 934, 777, 699, 147, 537, 907, 830, 112, 90, 218, 359, 283, 748, 428, 494, 899, 733, 154, 936, 169, 290, 468, 787, 683, 352, 704, 635, 743, 423, 71, 753, 400, 904, 465, 680, 462, 248, 711, 45, 825, 26, 526, 49, 929, 779, 453, 160, 505, 631, 387, 593, 806, 349, 701, 149, 544, 94, 874, 341, 572, 442, 251, 209, 182, 539, 757, 909, 177, 832, 667, 708, 278, 114, 769, 555, 639, 433, 281, 731, 902, 451, 92, 339, 207, 665, 276, 767, 449, 565, 567, 220, 361, 285, 61, 714, 77, 673, 736, 235, 478, 750, 430, 231, 117, 438, 864, 879, 295, 100, 496, 157, 569, 196, 689, 657, 485, 222, 796, 549, 848, 255, 363, 33, 287, 784, 724, 772, 268, 815, 152, 205, 447, 98, 547, 727, 310, 126, 917, 844, 316, 40, 67, 870, 142, 367, 892, 356, 776, 146, 536, 829, 217, 747, 427, 168, 824, 48, 592, 805, 730, 901, 275, 60, 76, 735, 234, 477, 230, 878, 156, 688, 656, 783, 723, 97, 39, 775, 828, 823, 591, 804, 729, 274, 877, 655, 273]\n"
     ]
    }
   ],
   "source": [
    "suffix_array, _ = construct_suffix_arrays(text)\n",
    "print(suffix_array.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi chúng ta đã xây dựng Suffix Array của một chuỗi Text, chúng ta có thể sử dụng nó để nhanh chóng xác định vị trí\n",
    "mỗi lần xuất hiện của một chuỗi Pattern trong Text. Đầu tiên, hãy nhớ lại rằng khi khớp Pattern với Suffix Trie, tất cả các khớp của Pattern trong Text phải xuất hiện tại điểm đầu tiên của các hậu tố của Text. Thứ hai là sau khi sắp xếp các Suffix của Text, các Suffix bắt đầu với một Pattern được nhóm lại với nhau theo trình tự alphabet. Ví dụ trong hình dưới đây Pattern = \"ana\" xuất hiện ở đầu các Suffix \"anamabananas\", \"anana\" và \"anas\" của Text = \"panamabananas\"; các hậu tố này xảy ra trong ba hàng liên tiếp và tương ứng với các vị trí bắt đầu 1, 7 và 9 trong Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu hỏi là làm thế nào để tìm các vị trí bắt đầu này cho một chuỗi Pattern tùy ý mà không cần lưu trữ các Suffix được sắp xếp của Text. Thuật toán PatternMatchingSuffixArray xác định chỉ mục đầu tiên và cuối cùng của Suffix Array tương ứng với các hậu tố bắt đầu bằng Pattern(các chỉ số này được ký hiệu first và last tương ứng). PatternMatchingSuffixArray là một biến thể của một kỹ thuật tìm kiếm chung gọi là tìm kiếm nhị phân (binary search), tìm một điểm dữ liệu trong một bộ dữ liệu đã được sắp xếp bằng cách lặp lại quá trình chia đôi dữ liệu và xác định nửa nào của bộ dữ liệu chứa điểm dữ liệu. Tuy nhiên thuật toán sẽ không chỉ ra được số lần lặp lại của điểm dữ liệu trong bộ dữ liệu, mà nó sẽ dừng lại khi tìm thấy vị trí đầu tiên của điểm dữ liệu trong bộ dữ liệu đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePatternMatchingSuffixArray(object):\n",
    "    \n",
    "    def __init__(self, text, patterns):\n",
    "        \n",
    "        self._text = text\n",
    "        self._patterns = patterns\n",
    "        self._suffix_array, self._text_patterns = self.construct_suffix_arrays()\n",
    "\n",
    "    def get_positions_of_patterns(self):\n",
    "        \n",
    "        position = []\n",
    "        for pattern in self._patterns:\n",
    "            first, last = self.match_pattern_to_suffix_array(pattern)\n",
    "            position.append([first, last])\n",
    "\n",
    "        return(position)\n",
    "        \n",
    "    def match_pattern_to_suffix_array(self, pattern):\n",
    "        \n",
    "        min_index = 0\n",
    "        max_index = len(self._text)\n",
    "        \n",
    "        while min_index < max_index:\n",
    "            mid_index = int((min_index + max_index)/2)\n",
    "            if pattern > self._text_patterns[mid_index]:\n",
    "                min_index = mid_index + 1\n",
    "            else:\n",
    "                max_index = mid_index\n",
    "        first = self._suffix_array[min_index]\n",
    "        last = first + len(pattern) - 1\n",
    "        \n",
    "        return(first, last)\n",
    "    \n",
    "    def construct_suffix_arrays(self):\n",
    "        text_patterns = []\n",
    "        for i in range(len(self._text)):\n",
    "            text_patterns.append(self._text[i::])\n",
    "\n",
    "        suffix_array = np.argsort(text_patterns)\n",
    "        text_patterns = sorted(text_patterns)\n",
    "\n",
    "        return(suffix_array, text_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'TTCTATCGGTTCGAAGTCGGATCTTGAGGGCGGTGGGGTCATTAGGGATACAGTCCATTATTACGCGCGGTTCGCACTTATGACAGTCGCTCCCCTAGCGTTAGCAGAGCACAGGCATCCGAGGTTTAAAGTTTGGGGTGGTGGAAATACTTAGCCAAGGGGGCGGAGTCCAACTGCGCGAGCTCAATCGACCGTTAGAGGCCTCCGGCCTCCGGACCGTTCACGAGATATGCATGCTCGGTTTTAGTACGCGCAGGTAAGTCCACAGTCAAGTTCGAGATGCCCGACAAACTCATCGAGGATTAGAACGGCCTCCCTTCCTGTTCGTTCTGTTAAGGTTCGGAACGCTCCTGGGCTCAAGATAGTGTTAGGCGGAATGTTCTAGGTTCTAGGTTGTATTACGCACTTTCAAAAAGAAATGGCCTCCGGCCTCCGGGTTCAGTGTTTACGCCGTTCCGACATTTTGGAATACACGAACCCATATTCTGCAAGGCAATCACTGCGCTCCGGGGATTTGCACGGTCATGAACTTTTCGGCCTCCGGCCTCCGGTTAATGTCCGAAGGTTCTGACTCGGATTTCTTCCCATGTGTGCTTTGATACACCCCGGAACGCGCTTCGAGACTACAGGCGACTGTCTATCTACAACAACACCCAGCCCTGTGAGCCTCAAGTCGAACATGATCTTTGTGCTTTGTGCTTTGCCCCGCTAACTCACCTTCACAGAAGGGACGTGAACCAATTAACCGCATGCTCCATGACAGGTATGGTGCCAACTGGACCATGGTTTCAAGGGTTTAACCGATATGGCAATGGCCGCTTTCGCTCTGATCTTTCCATTTAGTGACCAGAGAAGCAGGATACGATGGTTATATTAGGGCACCAATAGCGAGTTTAGGCACCATGACCCAGGGGTCGGTCTCGCCTATCGCCCCATTACTCCTAGTTAAATGGGGTGGCCTCCGGATACACGATTGGCGCCAGGCCCCAGGGTGTTATGTCATACTACAGTTCAATGGTCCTCACGAATCACGGCTTCCATGTCCAGCGGCCTCCGGCGCAATTAATGTGTAAGATGCTAACAAGGCGATCATAAGCATAGTGTAACTGGCCGCACCCGGAACTCTTCCCCATGACCCATGACCCCGTCATCGGTATCGTCCGCAGATGAAGACTGCGGAGAGATATGTTATGGCAGCTAATTTATAAGGTCGCTTGGCTGAACCAGGCCCCAGGCCCCCCCGTTCTAGGTGCACGTCATTGCAACGCCAATATTTCGAGTGGGGAGTTTCAATGCCGCCCTCTATATAGAGATTTCCGGCATACATCGGGTCATTATAACGCGGCGAAAACCCTCGTCCCAGGCCCCAGGCCCCGCGATTGAGTGGTGCGCAGGTGTTTCACATTATTAGGGCTAGTTGCCACATGGCCCCTGTCCACTGAACAGTGGGCAATCCGACAAATTTGATCTAGCCCGTCCCGAGAGCCAGAGGTAACTGCAGCCACTCTAGAGAAAGCATGGGACGGGAGAAGGTGACGGGAGACGGGAGACTGGCTTACCTGTATAAATCCGCCCTCCTCTAGACGCTTTGTTCAGTGTTAGGTTCGGCCTCCATTACCTCCCCTTTTAGGTGCATGACAGGGGAGCATCTTTCAACGTTCCATGACCCATGACCCGAACAGACGGTTTATGTGCTTTGTTGTATCTTCAACGCAAAAGACGTCTACGAGCTCCTTGAGGGAAGCGGACATCCGCTAATCCCGATCCCGCGAGGACAGAGCAATGCATGGCTTAGAAAACCGCCTACCTGAATTCATCATGCACACTCATGACCCAATACATACAGGTTACATTACCTATGATAGGCTTCAATGGTTTTATCCTAGAAACCCCAGGCCCCAGGCCCCTCGAACTGTTGGCCTTGGAACAACTTACGTTCTTATTTACTGTTCGTGGCATGATCGGCCACCTATAGGCACATTCCACCTTCGAAGAGACTGTCCCCAAAGCGCAGGCTACACTTCGCTAAATATTGCTCAGTGAGTGATGGTCTTCCAATCCTAAAGAGCTGAATATGTATGGCACTGATATAGAAGTCGAATCTATAAATACAATACCTTCATAATTGTGCTTTGTGCTTTGAAGCGGATACCGAAAAGCTCTGTGAAAAAACCACCATCACCTATTAGACGGGAGACGGGAGAAAGCATCGCGTCCCACTGCAAGGGGACGCCATGACCCTGATCAAGCGCAGCTCGAGCAGTACGTAAGTACAGTGACGGACGGGAGACGGGAGACCGAAATTCAGTATATCAGCAAGGCGGGTATCCATGAAAAAGTACGTGTCGTGGCCTGGTAGCTTAGTAGAGTCCTCAACGCTATCGGGTAGGACAATTTTAAATACTTAGAAATTGGGGTAGGGGACAACAACACGATTTTTTTCTGTGCTTTGTGCTTTGCGTTGAATCAAAAAAGCTTGCAAGACTACCTGGGCTCTCCCCACGGCGTCAGCTTGCAATCCCGTATCCAAAGCTTCGCCAAAGATGGGGCGTAGCCTGCTGTCACCGGCCGCATCTGAGACCAGTCTTCTTAAGTAGGGCGGCGCGCTACTTTTACAGCATTTCGACTTCAGTGTTGGCCGACTCCATCCAACCTGGTCACCGAACAGCGCGGGTGTTTACGCTTTCGCACCGCCTTGCAATTCGCCGGACTAAGCGTAATACGCCGGTTGAAAACGGTGCTGCAGCCCCGGAGGGTCCTAGACGTGCGACTACTATAGCTAGAGTTAATGTCATATCACCTGTTTCAGTGTTAACAGGCGATAAATAGTTACAGAAGGAAACATCAGGTAGAGGTTTC'\n",
    "patterns = ['GTTCTAGGT', 'GACGGGAGA', 'CCATGACCC', 'CCAGGCCCC','TGTGCTTTG', 'TTCAGTGTT', 'GGCCTCCGG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1242, 1250], [2195, 2203], [900, 908], [1223, 1231], [2133, 2141], [2804, 2812], [206, 214]]\n"
     ]
    }
   ],
   "source": [
    "positions = MultiplePatternMatchingSuffixArray(text, patterns).get_positions_of_patterns()\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result: \n",
      "GTTCTAGGT GACGGGAGA CCATGACCC CCAGGCCCC TGTGCTTTG TTCAGTGTT GGCCTCCGG\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing result: \")\n",
    "print(text[1242 : 1251], text[2195 : 2204], text[900 : 909], \n",
    "      text[1223 : 1232], text[2133 : 2142], text[2804 : 2813], text[206 : 215])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography: <br>\n",
    "[9] “Whole-Exome-Sequencing Identifies Mutations in Histone Acetyltransferase Gene KAT6B in Individuals with the Say-Barber-Biesecker Variant of Ohdo Sy... - PubMed - NCBI.” Accessed May 13, 2020. https://www.ncbi.nlm.nih.gov/pubmed/22077973.<br>\n",
    "[10] Tuzun, Eray, Andrew J. Sharp, Jeffrey A. Bailey, Rajinder Kaul, V. Anne Morrison, Lisa M. Pertz, Eric Haugen, et al. “Fine-Scale Structural Variation of the Human Genome.” <i>Nature Genetics</i> 37, no. 7 (July 2005): 727–32. https://doi.org/10.1038/ng1562.<br>\n",
    "[11] Montgomery, Stephen B., David L. Goode, Erika Kvikstad, Cornelis A. Albers, Zhengdong D. Zhang, Xinmeng Jasmine Mu, Guruprasad Ananda, et al. “The Origin, Evolution, and Functional Impact of Short Insertion-Deletion Variants Identified in 179 Human Genomes.” <i>Genome Research</i> 23, no. 5 (May 2013): 749–61. https://doi.org/10.1101/gr.148718.112.<br>\n",
    "[12] Aho, Alfred, and Margaret Corasick. “Efficient String Matching: An Aid to Bibliographic Search.” <i>Commun.</i> ACM 18 (June 1, 1975): 333–40. https://doi.org/10.1145/360825.360855.<br>\n",
    "[13] “(PDF) Linear Pattern Matching Algorithm.” Accessed May 13, 2020. https://www.researchgate.net/publication/229067733_Linear_Pattern_Matching_Algorithm.<br>\n",
    "[14] Manber, Udi, and Gene Myers. “Suffix Arrays: A New Method for On-Line String Searches.” <i>SIAM Journal on Computing</i> 22, no. 5 (October 1, 1993): 935–48. https://doi.org/10.1137/0222058."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Burrows-Wheeler Transform (BWT)<a name = 'bwt'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Genome Compression <a name = 'genome-compression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffix Array đã giảm đáng kể bộ nhớ cần thiết cho tìm kiếm Text và cho đến đầu thế kỷ này chúng vẫn thực sự tốt trong việc khớp Pattern. Chúng ta có thể tham vọng đến mức tìm kiếm một cấu trúc dữ liệu mã hóa Text với bộ nhớ chỉ xấp xỉ bằng độ dài của Text trong khi vẫn cho phép khớp Pattern nhanh chóng hay không?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để trả lời câu hỏi này, chúng ta sẽ xem xét chủ đề dường như không liên quan đến nén văn bản. Trong một kỹ thuật nén đơn giản được gọi là mã hóa độ dài chạy (run-length encoding), chúng ta thay thế một chuỗi k xuất hiện liên tiếp của ký hiệu s chỉ bằng hai ký hiệu: k, theo sau là s. Ví dụ, mã hóa độ dài chạy sẽ nén chuỗi TTTTTGGGAAAACCCCCCA thành 5T3G4A6C1A.\n",
    "Mã hóa độ dài chạy hoạt động tốt đối với các chuỗi có nhiều lần chạy dài, nhưng thực tế bộ gen không có nhiều lần chạy. Bộ gen chứa các kí tự lặp đi lặp lại không liên tiếp. Do đó, thật tuyệt nếu trước tiên chúng ta có thể chuyển sự lặp lại này thành các lần chạy và sau đó áp dụng mã hóa độ dài chạy cho chuỗi kết quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một cách ngây thơ trong việc tạo ra các chuỗi chạy là sắp xếp lại các ký tự trong chuỗi theo thứ tự alphabet. Ví dụ: TACGTAACGATACGAT sẽ trở thành AAAAACCCGGGTTTT, mà sau đó chúng ta có thể nén thành 5A3C3G4T. Tuy nhiên phương pháp này có vẻ chưa hợp lý. Nếu một chuỗi DNA GCATCATGCAT và ACTGACTACTG - cũng như bất kỳ chuỗi nào có cùng số lượng nucleotide - được sắp xếp lại thành AAACCCGGTTT thì chúng ta không thể giải nén chuỗi đã được nén, tức là đảo ngược quá trình nén để tạo ra chuỗi gốc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Constructing the Burrows-Wheeler transform <a name = constructing-bwt></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta hãy xem xét một phương pháp khác để chuyển đổi các lần lặp lại của chuỗi thành các lần chạy\n",
    "được đề xuất bởi Michael Burrows và David Wheeler vào năm 1994 [15]. Đầu tiên, tạo ra tất cả các vòng quay theo chu trình có thể có của Text; một vòng quay theo chu trình được xác định bằng cách cắt bỏ một Suffix từ cuối Text và nối thêm Suffix này vào đầu Text. Tiếp theo - tương tự như Suffix Array - sắp xếp tất cả các vòng quay theo chu trình của Text theo thứ tự alphabet để tạo thành một ma trận cỡ |Text|x|Text| của các ký tự mà chúng ta gọi là ma trận Burrows-Wheeler, kí hiệu M (Text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng cột đầu tiên của M (Text) chứa các ký hiệu của Text được sắp xếp theo alphabet, đây chỉ là sự sắp xếp lại ngây thơ của Text mà chúng ta đã mô tả. Đổi lại, cột thứ hai của M (Text) chứa các ký hiệu thứ hai của tất cả các vòng quay theo chu trình của Text và do đó nó cũng thể hiện sự sắp xếp lại (khác nhau) các ký tự từ Text.\n",
    "Tương tự, ta có thể chỉ ra rằng bất kỳ cột nào của M (Text) cũng là sự sắp xếp lại các ký tự của Text. Chúng ta quan tâm đến cột cuối cùng của M (Text), được gọi là Biến đổi Burrows-Wheeler của Text, hoặc BWT (Text), được hiển thị màu đỏ trong hình dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/IR6pFPU.jpg\" width=\"500\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta kiểm tra lại biến đổi Burrows-Wheeler ở hình trên, chúng ta sẽ nhận thấy ngay lập tức rằng nó đã tạo ra chiều dài chạy \"aaaaa\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/NARGytE.jpg\" width=\"400\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi biến đổi Burrows-Wheeler được áp dụng cho bộ gen, nó biến đổi nhiều lần lặp lại của bộ gen thành các lần chạy. Sau đó chúng ta có thể áp dụng một phương pháp nén bổ sung như mã hóa độ dài chạy để giảm hơn nữa bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bwt(text):\n",
    "    text_list  = []\n",
    "    bwt = []\n",
    "    for i in reversed(range(len(text))):\n",
    "        text_list.append(text[i::] + text[0:i])\n",
    "    sorted_list = sorted(text_list)\n",
    "    for pattern in sorted_list:\n",
    "        bwt.append(pattern[-1])\n",
    "\n",
    "    return(''.join(bwt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'CGGGGAGTTTAGATCCTATGGCGAGGCAACTTCGTCCCCGACTGACATTTGAGTAACGCCGTCATATCATAGCTTAGCCACGGACGCCTACTTGAAGAATGCGAAAGCGGCTGCACGAGCTGTCCCTCAGCCTCACTCCATTATGGCCGGAGCGGTCAATGCACCCCACGCCAGGCTCTAAACGACGCACTGTAAAAACTAGGTTGCCGCTTAAGAGAAGAGTGTGTTTCGTGCTTAGTGCCCCCTATTGGCGCTAGCAGACGGCGTTCGCCCAGGTTTTTAACCGACCCAGCACGGCCGACCCCTAGAGGGATTAAATGGGCAGTTTGATGATGGGTGGGTAATAGTCGATGTGGGTGACGCCCTCTACGCTTGTTCCCGCATCACGTGTGGCTTTCCGTAGCCTGCAGAAACATCCATGGCAGGCATAAAACGGCCGATAAGTAGTGAAGAACTAGCTATTCCTCGCTTGGTATTCTCTTATGACTTACGGCATGATCTCCTTGTCAGAGAGCGATCGGGGTGTTCCGATGTTTCGTGAACACCTTTTAGTCAAGGGGGTTTTTAACGGTACGGGTGGACCTGAAGCAAGTGTGGTCCGTCCACAAGTATCTCGCTGAAAAGGCCCATGCGCGGTGCATTCAGCTGTATCCTCTGCCCAAAGCTTCATTCCCGGCCCGAGATAAGATGACGATTGCTCTCTAGCTATGAGGCTATGTGCTGTAGTGCCTGGTGTGTGAAAAAACCTTAATGCATATCACTCCATGGTAGCTATAACGACTAGACAGTCACGCGCGGACTATATAATTTTTTGTAGATCTGTTATACGTGGGATTTACCCGGCGAGATAGTGAGGGTTAATTCGTATCTTTCAACTGACTGGGTAGGAGATTT$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TGATATAGGATAAGCATGATAATTATGACCGGTGTGAAACTCCTCTGATTCAGAGGCGTAGACAACGGCGAGCTCTCAGTATCGAAGCCGAGCGATCAGATCACGTAGGTTAGACTTCTGAGTTTCGTATGCAGCGAGTCGAATCTTTTTAGGCCTGGTTCAGTCCCTTCGTGGTGTGTTCGAAACCTTAGCTGGGCCCTATGTGGCACTGTAGTCGAGCTCGTTTGGGTCTTGCCAGGTGTGAGCCCCGTACTCGCGCCTTTGCAGGAGTACCGTTACCTGAGCCGTCGGCCTGTGCTCGCTCTAGGATAGCCAACCGAGCGTCAACATAAAAGGGTCTAAGCACAACAG$TAAGGCTCCTTTAAGTTCACGTAAGAGCGGCCCAATTCGCTGCTGCAAGCTACAGGGTGCAGGTGAAGCGTGCTTACTATTATAATCCGTCTGACGCTTAACCGAGCCTATTAGACAACTAATTCCGCAGGATTACTAGGGTCGTACTCGTGCGGTGCCAATTGGACTGCAACGCAAGAGTCGTAACATACCGTCCACGGGATGACACCCTCTTCAACTGATTCACGTTTCTGAGGCTGTCTGCGGTGGAAGTAGGTCGGATTACGAGATCCCTGATCAGAGAATCGTCGTTTCAGAGTGTTTCGATTAGGTGACTTAGTTAAGTTATCTGACCCTGCTGGCCAGCTAGATGCATCAAGGGTCTCCCGCTGGGAACGCTAGTGACCGTTGTTGATACGTCCATTTTCCCAACTCACAGCGGTCCAGCAAGTATAACCAGCGTGAATGGGAAATGGAGCGAATGCCTCCTCGAGAGGGCTGAGTATTCCGTCTCTCGCATACGAGTAGACTTACTTGAACTCCATTAGTCCGGAGTTTCTGGTA'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bwt(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Inverting the Burrows-Wheeler Transform<a name = 'inverting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như chúng ta đã đề cập, chúng ta sẽ không sử dụng một phương pháp nén Text mà không thể giải nén. Để ý rằng cột thứ nhất của M(Text) chính là cách nén ngây thơ mà chúng ta sẽ không thể giải nén được ra chuỗi gốc. Vậy các cột còn lại thì sao? Thật may mắn, các nhà khoa học đã chứng minh được rằng cột cuối cùng của M(Text) sẽ giải nén được thành chuỗi gốc ban đầu. Và từ cột cuối cùng này ta có thể dễ dàng biết được cột đầu tiên của M(Text) bằng cách xắp xếp lại chuỗi theo thứ tự từ vựng. Để xây dựng quá trình giải nén chúng ta sẽ xem xét các tính chất của Burrows-Wheeler Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Tính chất vòng quay chu trình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta hãy xem xét hàng đầu tiên của M(Text), kí tự $ nằm ở vị trí đầu tiên do cách sắp xếp theo thứ tự của cột đầu tiên, và ký tự này sẽ được đặt lên ngay trước kí tự thứ hai của hàng đầu tiên sau một vòng quay nào đó, vòng quay đó chính là hàng thứ tư của M(Text). Như vậy ta xác định được ký tự thứ hai của hàng đầu tiên là \"a\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/MASCoMI.jpg\" width=\"500\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thật dễ dàng để xác định ký tự \"a\" này, lý do là chỉ có duy nhất một ký tự \"$\" trong chuỗi Text. Để ý tại các hàng 7, 8, 9, 10 có đến bốn ký tự \"a\", vậy ta sẽ chọn \"b\", \"c\", hay \"d\" cho ký tự thứ ba của hàng đầu tiên? Để giải quyết vấn đề này ta hãy tìm hiểu tính chất tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Tính chất đầu cuối"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta thấy rằng, cột đầu tiên của M(Text) là một chuỗi được sắp xếp theo thứ tự từ vựng mà các chữ cái giống nhau được nhóm theo một cụm. Còn với cột cuối cùng, các chữ cái hoàn toàn không được nhóm theo một cụm, ví dụ \"r\" ở hàng hai và hàng năm. Tuy nhiên thứ tự của các ký tự giống nhau sẽ phải được bảo toàn ở cột cuối giống như cột đầu. Tính chất này được phát biểu như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Sự xuất hiện lần thứ k của một kí tự trong cột đầu và lần xuất hiện thứ k của ký tự này trong cột cuối tương ứng với cùng vị trí của ký tự này trong Text.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/JMy1BPi.jpg\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử các ký tự giống nhau được đánh chỉ số theo thứ tự của nó thì ta sẽ dễ dàng tìm được các ký tự còn lại của hàng đầu tiên. Mặt khác, các kí tự này có thể coi như các ký tự duy nhất trong chuỗi, giống với kí tự $. Từ đó ta có thể áp dụng quá trình đảo ngược này để tìm một chuỗi của bất kì hàng nào trong ma trận M(Text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/8kryCH0.jpg\" width=\"500\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta đi tìm chuỗi gốc. Để ý rằng có duy nhất một ký tự $ được đặt vào cuối của chuỗi Text, do đó chuỗi kí tự gốc chính là chuỗi ở hàng thứ tư của M(Text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseBurrowsWheelerTransform(object):\n",
    "    \n",
    "    def __init__(self, bwt):\n",
    "        self._bwt = bwt\n",
    "        # Starting Positions of Patterns matching Text.\n",
    "        self._start_positions = []\n",
    "        # Generate first column from BWT.\n",
    "        self._first_column = sorted(self._bwt)\n",
    "        # Keeping last column in string type to speed up runtime.\n",
    "        self._last_column_str = self._bwt\n",
    "        # Last column in list type.\n",
    "        self._last_column = list(self._bwt)\n",
    "        # Get symbols of the first column.\n",
    "        self._alphabet = np.unique(self._first_column)\n",
    "        \n",
    "        # Adding index to the first column.\n",
    "        self._index_first_column = self.index_symbol_of_column(self._first_column)\n",
    "        # Adding the same index of corresponding symbols to the last column.\n",
    "        self._index_last_column = self.index_symbol_of_column(self._last_column)\n",
    "        # Get index of the first symbol of the first column.\n",
    "        self._first_occurence = self.get_first_occurence()\n",
    "        \n",
    "    # Inverting a bwt to original text\n",
    "    def invert_bwt(self):\n",
    "        \n",
    "        org_string = []\n",
    "        length = len(self._index_first_column)\n",
    "        \n",
    "        # Find row concluding symbol (0, '$') at last.\n",
    "        index_str = self._index_last_column.index((0, '$'))\n",
    "        \n",
    "        while 1 < 2:\n",
    "            # Finding the first symbol of the row above\n",
    "            org_symbol_of_string = self._index_first_column[index_str]\n",
    "            # Only choosing symbol, excluding index.\n",
    "            org_string.append(org_symbol_of_string[1])\n",
    "            # The loop is going on.\n",
    "            index_str = self._index_last_column.index(org_symbol_of_string)\n",
    "            # If the loop returns symbol (0, '$'), we stop the loop.\n",
    "            if org_symbol_of_string == (0, '$'):\n",
    "                break\n",
    "                \n",
    "        return(''.join(org_string))\n",
    "    # Find the number of patterns matching text by bwt.\n",
    "    def match_multi_patterns_bwt(self, patterns):\n",
    "        number = []\n",
    "        for pattern in patterns:\n",
    "            number.append(self.match_bwt(pattern))\n",
    "            \n",
    "        return(number)\n",
    "    # Find the number of patterns matching text by better bwt.\n",
    "    def match_multi_patterns_better_bwt(self, patterns):\n",
    "        number = []\n",
    "        for pattern in patterns:\n",
    "            number.append(self.get_better_bw_matching(pattern))\n",
    "            \n",
    "        return(number)\n",
    "    # Find the number of patterns matching text by better bwt with checkpoints.\n",
    "    def match_multi_patterns_better_bwt_with_check_points(self, patterns, c):\n",
    "        number = []\n",
    "        self._index_check_point, self._count_dict = self.get_check_point_arrays(c)\n",
    "        for pattern in patterns:\n",
    "            number.append(self.get_better_bw_matching_with_check_points(pattern))\n",
    "            \n",
    "        return(number)\n",
    "    # Find positions of text that patterns matching the text by better bwt.\n",
    "    def locate_multi_patterns_better_bwt(self, patterns, partial_suffix_array, suffix_array_index):\n",
    "        positions = []\n",
    "        for pattern in patterns:\n",
    "            positions.append(self.locate_better_bw_matching(pattern, partial_suffix_array, suffix_array_index))\n",
    "            \n",
    "        return(positions)\n",
    "    \n",
    "    # Find positions of text that patterns matching the text by better bwt with check point.\n",
    "    def locate_multi_patterns_better_bwt_with_check_points(self, patterns, c, partial_suffix_array, suffix_array_index):\n",
    "        positions = []\n",
    "        self._index_check_point, self._count_dict = self.get_check_point_arrays(c)\n",
    "        for pattern in patterns:\n",
    "            positions.append(self.locate_better_bw_matching_with_check_points(pattern, c, partial_suffix_array, suffix_array_index))\n",
    "            \n",
    "        return(positions)\n",
    "    # Find the number of pattern matching text by bwt.\n",
    "    def match_bwt(self, pattern):\n",
    "        # Set the initial top and bottom.\n",
    "        top = 0\n",
    "        l = len(self._last_column_str)\n",
    "        bottom = l - 1\n",
    "        \n",
    "        while top <= bottom:\n",
    "            if pattern != '':\n",
    "                symbol = pattern[-1] # Moving backward of Pattern.\n",
    "                pattern = pattern[0 : -1] # Remove the last symbol of pattern.\n",
    "               \n",
    "                if symbol in self._last_column_str[top : (bottom + 1)]:\n",
    "                    # Finding top_index and bottom_index of next symbol at the last column.\n",
    "                    top_index = self._last_column_str[top : (bottom + 1)].index(symbol) + top\n",
    "                    bottom_index = bottom - list(reversed(self._last_column_str[top : (bottom + 1)])).index(symbol)\n",
    "                    # Updating top and bottom.\n",
    "                    top = self.get_last_to_first(top_index)\n",
    "                    bottom = self.get_last_to_first(bottom_index)\n",
    "                    \n",
    "                else:\n",
    "                    return(0)\n",
    "            else:\n",
    "                # Return number of positions of text that pattern matches text at that positions.\n",
    "                return(bottom - top + 1) \n",
    "    # Find the number of pattern matching text by better bwt.\n",
    "    def get_better_bw_matching(self, pattern):\n",
    "        # Set the initial top and bottom.\n",
    "        top = 0\n",
    "        bottom = len(self._last_column_str) - 1\n",
    "        \n",
    "        while top <= bottom:\n",
    "            if pattern != '':\n",
    "                symbol = pattern[-1] # Moving backward of Pattern.\n",
    "                pattern = pattern[0 : -1] # Remove the last symbol of pattern.\n",
    "                # Updating top and bottom.\n",
    "                top = self._first_occurence[symbol] + self.count_symbol(symbol, top)\n",
    "                bottom = self._first_occurence[symbol] + self.count_symbol(symbol, bottom + 1) - 1\n",
    "            else:\n",
    "                return(bottom - top + 1)\n",
    "        return(0)\n",
    "    # Find the number of pattern matching text by better bwt with checkpoints.\n",
    "    def get_better_bw_matching_with_check_points(self, pattern):\n",
    "        # Set the initial top and bottom.\n",
    "        top = 0\n",
    "        bottom = len(self._last_column_str) - 1\n",
    "        \n",
    "        while top <= bottom:\n",
    "            \n",
    "            if pattern != '':\n",
    "                symbol = pattern[-1] # Moving backward of Pattern.\n",
    "                pattern = pattern[0 : -1] # Remove the last symbol of pattern.\n",
    "                # Updating top and bottom with checkpoints.\n",
    "                if top not in self._index_check_point:\n",
    "                    top = self._first_occurence[symbol] + self.count_symbol(symbol, top)\n",
    "                else:\n",
    "                    top = self._first_occurence[symbol] + self._count_dict[top][symbol]\n",
    "\n",
    "                if (bottom + 1) not in self._index_check_point:\n",
    "                    bottom = self._first_occurence[symbol] + self.count_symbol(symbol, bottom + 1) - 1\n",
    "                else:\n",
    "                    bottom = self._first_occurence[symbol] + self._count_dict[bottom + 1][symbol] - 1\n",
    "\n",
    "            else:\n",
    "                return(bottom - top + 1)\n",
    "        return(0)\n",
    "   \n",
    "    # Find positions of text that pattern matching the text by better bwt.\n",
    "    def locate_better_bw_matching(self, pattern, partial_suffix_array, suffix_array_index):\n",
    "        # Set the initial top and bottom.\n",
    "        top = 0\n",
    "        bottom = len(self._last_column_str) - 1\n",
    "        starting_positions = []\n",
    "      \n",
    "        while top <= bottom:\n",
    "            if pattern != '':\n",
    "                symbol = pattern[-1] # Moving backward of Pattern.\n",
    "                pattern = pattern[0 : -1] # Remove the last symbol of pattern.\n",
    "                # Updating top and bottom.\n",
    "                top = self._first_occurence[symbol] + self.count_symbol(symbol, top)\n",
    "                bottom = self._first_occurence[symbol] + self.count_symbol(symbol, bottom + 1) - 1\n",
    "            else:\n",
    "                # Continuously backward to find the locations.\n",
    "                for i in range(top, bottom + 1):\n",
    "    \n",
    "                    count = 0\n",
    "                    last_index = i\n",
    "                    \n",
    "                    while last_index not in suffix_array_index:\n",
    "                        \n",
    "                        count += 1\n",
    "                        symbol = self._last_column[last_index]\n",
    "                        last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "                      \n",
    "                    starting_positions.append(partial_suffix_array[last_index] + count)\n",
    "                    \n",
    "                return(sorted(starting_positions))\n",
    "            \n",
    "        return('None')\n",
    "    # Get approximatly patterns matching text by better bwt.\n",
    "    def get_approximate_pattern_matching_with_better_bwt(self, patterns, d, partial_suffix_array, suffix_array_index):\n",
    "        \n",
    "        starting_positions = []\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            \n",
    "            self._start_positions = []\n",
    "            # Set initial top and bottom.\n",
    "            top = 0\n",
    "            l = len(self._last_column)\n",
    "            bottom = l - 1\n",
    "            \n",
    "            \"\"\" If we want to assign the initial value of the pattern to match the symbol then run the hidden code folows:\"\"\"\n",
    "            #symbol = pattern[-1]\n",
    "            #pattern = pattern[0 : -1]\n",
    "\n",
    "            #if symbol in self._last_column[top : (bottom + 1)]:\n",
    "\n",
    "                #top_index = self._last_column[top : (bottom + 1)].index(symbol) + top\n",
    "                #bottom_index = bottom - list(reversed(self._last_column[top : (bottom + 1)])).index(symbol)\n",
    "                #top = self.get_last_to_first(top_index)\n",
    "                #bottom = self.get_last_to_first(bottom_index)\n",
    "                \n",
    "            starting_positions.append(self.get_sub_aproximate_string(pattern, top, bottom, d, 0, \n",
    "                                                                     partial_suffix_array, suffix_array_index)\n",
    "                                     )\n",
    "        return(starting_positions)\n",
    "    # Get approximatly patterns matching text by better bwt with checkpoints.\n",
    "    def get_approximate_pattern_matching_with_check_points(self, patterns, c, d, partial_suffix_array, suffix_array_index):\n",
    "        \n",
    "        starting_positions = []\n",
    "        self._index_check_point, self._count_dict = self.get_check_point_arrays(c)\n",
    "      \n",
    "        for pattern in patterns:\n",
    "            self._start_positions = []\n",
    "            # Set initial top and bottom.\n",
    "            top = 0\n",
    "            l = len(self._last_column)\n",
    "            bottom = l - 1\n",
    "            \n",
    "            \"\"\" If we want to assign the initial value of the pattern to match the symbol then run the hidden code folows:\"\"\"\n",
    "            #symbol = pattern[-1]\n",
    "            #pattern = pattern[0 : -1]\n",
    "\n",
    "            #if symbol in self._last_column[top : (bottom + 1)]:\n",
    "\n",
    "                #top_index = self._last_column[top : (bottom + 1)].index(symbol) + top\n",
    "                #bottom_index = bottom - list(reversed(self._last_column[top : (bottom + 1)])).index(symbol)\n",
    "                #top = self.get_last_to_first(top_index)\n",
    "                #bottom = self.get_last_to_first(bottom_index)\n",
    "                \n",
    "            starting_positions.append(self.get_sub_aproximate_string_with_check_points(pattern, top, bottom, d, 0, \n",
    "                                                                                       partial_suffix_array, suffix_array_index)\n",
    "                                     )\n",
    "        return(starting_positions)\n",
    "    # Get approximatly pattern matching text by better bwt.\n",
    "    def get_sub_aproximate_string(self, pattern, top, bottom, d, initial_mismatch, \n",
    "                                  partial_suffix_array, suffix_array_index\n",
    "                                 ):\n",
    "        # Get symbol exiting from top to bottom of the last column.\n",
    "        apr_symbol_alphabet = ''.join(np.unique(self._last_column[top : (bottom + 1)]))\n",
    "        # If pattern equals '', we find the positions of text that pattern matches the text.\n",
    "        if pattern == '':\n",
    "            \n",
    "            for i in range(top, bottom + 1):\n",
    "                # Set the initial backward count.\n",
    "                count = 0\n",
    "                last_index = i\n",
    "                # When partial suffix array is not exist, we go backward again.\n",
    "                while last_index not in suffix_array_index:\n",
    "                    \n",
    "                    count += 1\n",
    "                    symbol = self._last_column_str[last_index]\n",
    "                    last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "            # Starting position of pattern equals sum of partial suffix array and extend backward count.        \n",
    "            self._start_positions.append(partial_suffix_array[last_index] + count)\n",
    "            \n",
    "        else:\n",
    "            symbol = pattern[-1] # Moving backward of Pattern.\n",
    "            pattern = pattern[:-1] # Remove the last symbol of pattern.\n",
    "            for apr_symbol in apr_symbol_alphabet:\n",
    "                # Set the initial mismatch.\n",
    "                mismatch = initial_mismatch\n",
    "                \n",
    "                if apr_symbol != symbol:\n",
    "                    mismatch += 1\n",
    "                # If mismatch <= d, we update top and bottom.\n",
    "                if mismatch <= d:   \n",
    "                    \n",
    "                    new_top = top\n",
    "                    new_bottom = bottom\n",
    "                    \n",
    "                    \"\"\" If we want to use normal bwt instead of better bwt then run the hidden code folows:\"\"\"\n",
    "                    #top_index = self._last_column[new_top : (new_bottom + 1)].index(apr_symbol) + new_top\n",
    "                    #bottom_index = new_bottom - list(reversed(self._last_column[new_top : (new_bottom + 1)])).index(apr_symbol)\n",
    "                    #new_top = self.get_last_to_first(top_index)\n",
    "                    #new_bottom = self.get_last_to_first(bottom_index)\n",
    "                    \n",
    "                    new_top = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_top)\n",
    "                    new_bottom = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_bottom + 1) - 1\n",
    "                    # Using recurence algorithm for the new patterns that are exact or in-exact matching text.\n",
    "                    self.get_sub_aproximate_string(pattern, new_top, new_bottom, d, \n",
    "                                                                    mismatch, partial_suffix_array, suffix_array_index\n",
    "                                                                    ) \n",
    "        return(self._start_positions)\n",
    "    # Get approximatly pattern matching text by better bwt with checkpoints.\n",
    "    def get_sub_aproximate_string_with_check_points(self, pattern, top, bottom, d, initial_mismatch, \n",
    "                                  partial_suffix_array, suffix_array_index\n",
    "                                 ):\n",
    "        # Get symbol exiting from top to bottom of the last column.\n",
    "        apr_symbol_alphabet = ''.join(np.unique(self._last_column[top : (bottom + 1)]))\n",
    "        # If pattern equals '', we find the positions of text that pattern matches the text.\n",
    "        if pattern == '':\n",
    "            \n",
    "            for i in range(top, bottom + 1):\n",
    "                # Set the initial backward count.\n",
    "                count = 0\n",
    "                last_index = i\n",
    "\n",
    "                while last_index not in suffix_array_index:\n",
    "\n",
    "                    count += 1\n",
    "                    symbol = self._last_column_str[last_index]\n",
    "                    last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "                    \n",
    "            self._start_positions.append(partial_suffix_array[last_index] + count)\n",
    "            \n",
    "        else:\n",
    "            symbol = pattern[-1]\n",
    "            pattern = pattern[:-1]\n",
    "            for apr_symbol in apr_symbol_alphabet:\n",
    "                # Set the initial mismatch.\n",
    "                mismatch = initial_mismatch\n",
    "                \n",
    "                if apr_symbol != symbol:\n",
    "                    mismatch += 1\n",
    "                # If mismatch <= d, we update top and bottom.\n",
    "                if mismatch <= d:   \n",
    "                    \n",
    "                    new_top = top\n",
    "                    new_bottom = bottom\n",
    "                    \n",
    "                    \"\"\" If we want to use normal bwt instead of better bwt then run the hidden code folows:\"\"\"\n",
    "                    #top_index = self._last_column[new_top : (new_bottom + 1)].index(apr_symbol) + new_top\n",
    "                    #bottom_index = new_bottom - list(reversed(self._last_column[new_top : (new_bottom + 1)])).index(apr_symbol)\n",
    "                    #new_top = self.get_last_to_first(top_index)\n",
    "                    #new_bottom = self.get_last_to_first(bottom_index)\n",
    "                    \n",
    "                    #new_top = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_top)\n",
    "                    #new_bottom = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_bottom + 1) - 1\n",
    "                   \n",
    "                    # Updating top and bottom using checkpoints.\n",
    "                    if new_top not in self._index_check_point:\n",
    "                        new_top = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_top)\n",
    "                    else:\n",
    "                        new_top = self._first_occurence[apr_symbol] + self._count_dict[new_top][apr_symbol]\n",
    "                    \n",
    "                    if new_bottom + 1 not in self._index_check_point:\n",
    "                        new_bottom = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_bottom + 1) - 1 \n",
    "                    else:\n",
    "                        new_bottom = self._first_occurence[apr_symbol] + self._count_dict[new_bottom + 1][apr_symbol] - 1\n",
    "                     # Using recurence algorithm for the new patterns that are exact or in-exact matching text.\n",
    "                    self.get_sub_aproximate_string(pattern, new_top, new_bottom, d, \n",
    "                                                                    mismatch, partial_suffix_array, suffix_array_index\n",
    "                                                                    ) \n",
    "        return(self._start_positions)\n",
    "    # Find positions of text that pattern matches the text.\n",
    "    def locate_better_bw_matching_with_check_points(self, pattern, c, partial_suffix_array, suffix_array_index):\n",
    "        # Set the initial top and bottom\n",
    "        top = 0\n",
    "        bottom = len(self._last_column_str) - 1\n",
    "        starting_positions = []\n",
    "        \n",
    "        while top <= bottom:\n",
    "            if pattern != '':\n",
    "                symbol = pattern[-1] # Go backward through the pattern.\n",
    "                pattern = pattern[0 : -1] # Remove the last symbol of pattern.\n",
    "                # Updating top and bottom with checkpoints.\n",
    "                if top not in self._index_check_point:\n",
    "                    top = self._first_occurence[symbol] + self.count_symbol(symbol, top)\n",
    "                else:\n",
    "                    top = self._first_occurence[symbol] + self._count_dict[top][symbol]\n",
    "                    \n",
    "                if bottom + 1 not in self._index_check_point:\n",
    "                    bottom = self._first_occurence[symbol] + self.count_symbol(symbol, bottom + 1) - 1\n",
    "                else:\n",
    "                    bottom = self._first_occurence[symbol] + self._count_dict[bottom + 1][symbol] - 1\n",
    "            else:\n",
    "\n",
    "                for i in range(top, bottom + 1):\n",
    "                    # Set the initial extend backward count.\n",
    "                    count = 0\n",
    "                    last_index = i\n",
    "                    # If partial suffix array doesn't exist, we go backward again.\n",
    "                    while last_index not in suffix_array_index:\n",
    "\n",
    "                        count += 1\n",
    "                        symbol = self._last_column_str[last_index]\n",
    "                        last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "\n",
    "                    starting_positions.append(partial_suffix_array[last_index] + count)\n",
    "\n",
    "                return(sorted(starting_positions))\n",
    "\n",
    "        return('None')\n",
    "    # Adding index for each symbol.\n",
    "    def index_symbol_of_column(self, column):\n",
    "       \n",
    "        l = len(column)\n",
    "        col = column[::]\n",
    "        \n",
    "        for x in self._alphabet:\n",
    "            count = 0\n",
    "            for i in range(l):\n",
    "                # Index equals occurence order of symbol.\n",
    "                if col[i] == x:\n",
    "                    count = count + 1\n",
    "                    col[i] = (count, x)\n",
    "                    \n",
    "        return(col) \n",
    "    # Finding index of symbol in the first column coresponds to the same symbol in the last column.\n",
    "    def get_last_to_first(self, last_index):\n",
    "        \n",
    "        first_index = self._index_first_column.index(self._index_last_column[last_index])\n",
    "        \n",
    "        return (first_index)\n",
    "    # Finding index of the symbol occuring at first in the first column.\n",
    "    def get_first_occurence(self):\n",
    "        first_occurence = {}\n",
    "        for x in self._alphabet:\n",
    "            first_occurence.update({x : self._first_column.index(x)})\n",
    "        return(first_occurence)\n",
    "    # Count symbol from 0 to index.\n",
    "    def count_symbol(self, symbol, index):\n",
    "        \n",
    "        return(self._last_column_str.count(symbol, 0, index))\n",
    "    # Construct checkpoints arrays.\n",
    "    def get_check_point_arrays(self, c):\n",
    "        \n",
    "        length = len(self._last_column)\n",
    "        index_check_point = np.arange(0, length + 1, c).tolist()\n",
    "        count_dict = {}\n",
    "\n",
    "        for i in index_check_point: \n",
    "            check_point_arrays = {}\n",
    "            instance_column = self._last_column[0: i]\n",
    "            # Finding number of symbols' occurence from 0 to i.\n",
    "            count_dict.update({i:Counter(instance_column)})\n",
    "        \n",
    "        return(index_check_point, count_dict)   \n",
    "\n",
    "    def divide_pattern(self, pattern, d):\n",
    "        sub_strings = []\n",
    "        k = int(len(pattern)/(d+1))\n",
    "        for i in range(0, d):\n",
    "            sub_strings.append(pattern[i*k: ((i+1)*k)])\n",
    "        sub_strings.append(pattern[d*k::])\n",
    "        return(sub_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt = 'TGCTCAGGCAATCTCATAGAAGATCTATGATTAAACTGTCTCGGGAGGAAGTCGTGGCTACATCACATAAGATGGAATAAACAGTATACATACCGCCAGTTCGGCAATGCTGTCGACTTGAAGCTGCGTCATGGCCAAGACAACTTCGTGCAACGTTGATGACTACTTGAAGGGACCCAACGCTAGAGGCAGTGCTCTCGGTACTGGTAATCCACGAATCAAAAATGCCTGATTGCGCGAGTGTTTAGTGGGACCCATGAGTAATTTCAGTGAACGACACGCTGATGTACTAGTTCTTCCCTAAATTACGAGCGAACCTATAGAGTACGTTGCCTTGGTGACCGCCTGCGGGCACAACGATAGTATGTTTGCAGAAACTGCGACCGTGGCAGATAGCGAACAGGCCAGGTGCTCAGATGCCACTCGAAGTCGCTGCACTAAGTCACGACCTCCGGCGGCAACGCTGCACTTGGCTAGTCACTGGCACATCTAGTCGGAAAGGATATTCACTACACTAGAATCCATGATCGCCCAGCCGTAACGGTCTGGAGGACCGCAGTGCTACTTTGTGTAAATACTAAAATCTGCCAAGGGTCGGCGAATTCTCGTAGAGGGGGCCGGGTAATGGCCTAATCTGCGCCTTAGCTGTCATTGCTGTTGGCGAGTTAAACTATTCTTACATCTTGTACATCAGCTTTTGAGGGGCGATGTATTCTAATCTATCCTGTGCGTCTCCTCACTGCGCAGATCGTATATCGTCTCGGTACTATGCATCCGTGTTACCCGTCTGAAGATAATGGCGAGACACTT$AACCAGCATGGTGCGTCCCGGCCATGGCCGTGCACAACGAGGTATCTCCGAAGTAGACAGTCAAATGCGTACTTTCCTCTAATATTGCGTGGCGAAGTATAGTGAATTTAGATGAGATAGTTACAGCTCGGGAGGGTAATCCGTAAGGCCTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCCTGTGACGGGCAGGAACACTGCTGCCGGGCAATCGAAGACGGACAAGGGACGCACGTGCCTGGGTGTACACCCAGATCAGCTATGGACAGTACCAGTTTATTGGCGAAGAGCGCAAAGAGGCGAGTTAATGCACGCTACGATGCCTCGCCTAAATTCTCCGCGTTAAAACAGAGGGGCAGCAGTAGAGGGTTAACACTCCACTTTTAGCGCCCTGAAATGGTAACACCAAGACCTTAAACCGTCGATTAAGAAAATGGGCTGACTCTGAAAAATCATGAAGCCGATGTGGAACGGTGCCTAGTGAGTCTTGGTGTTGAAGTAAAGCGTAGCGGGCCGGGTGTTTGCGAGAAGTCATTCCGGGGGCCCTAACGATCCCTGCGTTTGTAGCTCCGAGCAAACCATTATTTACCTGCAAAACACGAGCGCTAGCGCCTCGAAGCTTTTTCATCATGCAGACCCGCCATGTTAGAGTTGTCTCAAGTTCATGGCTCTATCCCGATTGCGCGGACGAAATCGACTGGGCAATGGGATTGTGCCTTTATTACTGTTGTCAGTCCCCCATGTGGAGGGATCCGCCCGTGAGACTAATTTGAGCGGTTAGGATACGTTGGGGTCTAGAAGAGAAGCCATGATCAGTACGCTGCGGCCAGCCTCAAAAGCGGGCCGGCATATTTACTAACATATAGGATTCACCCTATGTATTAACCAACTATTTCTAACTACAGTGAATTACGGCTGTAATGGTAAACCAGAGATTATATCGAAAATGGAAGTGGTGTTGTGGCTTTATCGCTTGTCACGGCTGTCGTTGTTCACCCCTCTGTCGTCTTTGTTGTTCCTAGCAAAATGGTGGGCTGATACCAACGAGTTGTCCCTAAGGCTAATAAGTATCATACTCGACAACAATGCGATTCGTCACAGACCTCTTGCGTCCACCTGTATTATACCTGACTCACCCT$'"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InverseBurrowsWheelerTransform(bwt).invert_bwt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Pattern Matching with the Burrows-Wheeler Transform<a name = 'pattern-matching'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 A first attempt at Burrows-Wheeler pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta đã biết có thể xây dựng lại ma trận M(Text) mà chỉ cần biết duy nhất cột cuối cùng của nó hay còn gọi là Burrows-Wheeler Transform. Để ý rằng ma trận M(Text) có các hàng chứa các Suffix mà chạy từ điểm đầu tiên đến vị trí của $. Tuy nhiên chúng ta không thể lưu trữ toàn bộ M(Text) nếu Text quá lớn như bộ gen người. Do đó, chúng ta sẽ tìm cách hay vì lưu lại toàn bộ M(Text) ta chỉ cần lưu vị trí bắt đầu của Suffix trong Text, hay chính là Suffix Array. Dựa vào Burrows-Wheeler Transform của M(Text) ta có thể xây dựng được thuật toán khớp một Pattern vào Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/iRsz1u5.jpg\" width=\"500\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Moving backward through a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như đã biết, từ cột Burrows-Wheeler Transform ta có thể xây dựng được cột đầu tiên của M(Text), từ hai cột nay ta xây dựng thuật toán khớp một Pattern vào Text bằng cách di chuyển lùi qua Pattern. Ví dụ, nếu chúng ta muốn khớp mẫu = \"ana\" với Text = \"panamabananas$\", trước tiên chúng ta sẽ xác định các hàng của M (Text) bắt đầu bằng \"a\", chữ cái cuối cùng của \"ana\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/9W9CQgs.jpg\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi chúng ta đang di chuyển ngược qua \"ana\", chúng ta sẽ tìm các hàng của M (Text) bắt đầu bằng \"na\". Để làm điều này mà không cần biết toàn bộ ma trận M (Text), thực tế là một kí tự trong cột cuối (LastColumn) phải đứng trước kí tự của Text được tìm thấy trong cùng một hàng của cột đầu (FirstColumn). Vì vậy, chúng ta chỉ cần xác định các hàng M (Text) bắt đầu bằng \"a\" và kết thúc bằng \"n\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/R3GyQ5a.jpg\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính chất đầu cuối cho chúng ta biết nơi tìm thấy ba chữ \"n\" được tô sáng trong FirstColumn, như hình dưới đây tất cả ba hàng kết thúc bằng \"a\", mang lại ba lần xuất hiện của \"ana\" trong Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/6XwP7jS.jpg\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sự xuất hiện nhiều lần của \"a\" trong LastColumn tương ứng với lần thứ ba, thứ tư và lần xuất hiện thứ năm của \"a\" trong cột này, tính chất đầu cuối cho chúng ta biết rằng chúng phải tương ứng với lần xuất hiện thứ ba, thứ tư và thứ năm của \"a\" trong FirstColumn, vì vậy chúng ta có thể xác định được ba vị trí khớp của của \"ana\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/1SBWsQP.jpg\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 The Last-to-First mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta đã biết cách sử dụng BWT (Text) để tìm tất cả các khớp của Pattern trong Text bằng cách quay lui qua pattern. Tuy nhiên, mỗi khi chúng ta quay lui, chúng ta cần phải theo dõi các hàng của M (Text) nơi chứa các khớp của hậu tố Pattern. Chúng ta biết rằng ở mỗi bước các hàng M (Text) khớp với một hậu tố của Pattern gom lại với nhau trong các hàng liên tiếp của M (Text). Điều này có nghĩa là tập hợp của tất cả các hàng khớp chỉ cần biểu diễn bởi hai con trỏ, trên cùng (top) và dưới cùng (bottom): top là chỉ mục của hàng đầu tiên của M (Text) khớp với hậu tố hiện tại của Pattern và bottom là chỉ mục của hàng cuối cùng của M (Text) khớp với hậu tố này. Hình dưới đây mô tả sự thay đổi của top và bottom trong quá trình khớp Pattern = \"ana\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/2rfXZiu.jpg\" width=\"800\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định các top và bottom ta cần thực hiện vòng lặp trong quá trình quay lui Pattern như sau:<br><br>\n",
    "topindex $\\gets$ vị trí đầu tiên của kí tự thuộc Pattern match với một kí tự ở cột cuối giữa các vị trí top và bottom ở cột cuối<br>\n",
    "bottomindex $\\gets$ vị trí cuối cùng của kí tự thuộc Pattern match với một kí tự ở cột cuối giữa các vị trí top và bottom ở cột cuối<br>\n",
    "top $\\gets$ LasttoFirst (topindex)<br>\n",
    "bottom $\\gets$ Lasttofirst (bottomindex)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bảng dưới đây, cột Last to First (i) cho thấy giá trị của các top và bottom ở các bước tiếp theo sau khi đã xác định được vị trí của các kí tự ở cột cuối của M(Text) của bước trước đó. Ta cũng có thể tính được số lần khớp của Pattern bằng bottom - top + 1. Cột Count đếm số kí tự xuất hiện từ đầu tiên cho đến trước các vị trí ở cột Last to First (i). Chúng ta sẽ thấy các giá trị của cột Count tham gia vào thuật toán ở phần sau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/d8Lxz3R.jpg\" width=\"800\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Data Science/Data/Mutations/How Do We Locate Disease/Multi_patterns_bwt/dataset_301_7.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    f.close\n",
    "bwt = lines[0]\n",
    "patterns = lines[1].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bwt:  20001\n",
      "Number of patterns:  3915\n",
      "Number of substring matches of the i-th member of patterns in text: \n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "--- 26.802515983581543 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Length of bwt: ', len(bwt))\n",
    "print('Number of patterns: ', len(patterns))\n",
    "print('Number of substring matches of the i-th member of patterns in text: ')\n",
    "start_time = time.time()\n",
    "print(InverseBurrowsWheelerTransform(bwt).match_multi_patterns_bwt(patterns))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Speeding Up Burrows-Wheeler Pattern Matching<a name = 'speed-up'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở phần trước, khi đi tìm các top và bottom ta cần so sánh các topindex và bottomindex với các kí tự ở cột đầu tiên, sự so sánh này sẽ làm tăng thời gian tính toán đồng thời làm tăng yêu cầu của bộ nhớ do phải lưu tất cả các kí tự ở cột đầu tiên. Giải pháp để làm giảm chi phí ở đây là chúng ta chỉ lưu kí tự và vị trí đầu tiên của nó ở cột đầu tiên, ta có thể xây dựng hàm $FirstOccurence (symbol)$ để tìm vị trí đầu tiên của kí tự symbol ở cột đầu tiên. Dựa vào tính chất đầu cuối, vị trí của symbol ở cột đầu ở bước sau sẽ được tính bằng $FirstOccurence (symbol)$ cộng với số lần xuất hiện symbol trước vị trí của nó ở cột cuối của bước trước. Số lần xuất hiện này có thể được tính bằng hàm $Count(symbol, i)$. $$top \\gets FirstOccurence(Symbol) + Count(symbol, top)\\\\\n",
    "bottom \\gets FirstOccurence(Symbol) + Count(symbol, bottom + 1) - 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bwt:  20001\n",
      "Number of patterns:  3915\n",
      "Number of substring matches of the i-th member of patterns in text: \n",
      "--- 2.6984572410583496 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Length of bwt: ', len(bwt))\n",
    "print('Number of patterns: ', len(patterns))\n",
    "print('Number of substring matches of the i-th member of patterns in text: ')\n",
    "InverseBurrowsWheelerTransform(bwt).match_multi_patterns_better_bwt(patterns)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù thuật toán của chúng ta là tăng tốc Burrows-Wheeler Pattern Matching, nhưng có vẻ ta vẫn còn có thể làm cho thuật toán nhanh hơn. Lý do ở đây là chúng ta đang phải tính số lần xuất hiện của symbol sau mỗi vòng lặp bằng hàm $Count(symbol, i)$. Để làm cho runtime nhanh hơn ta có thể tính trước số lần xuất hiện của symbol như cột Count ở phần trước, sau đó chỉ cần gọi lại kết quả này. Tuy nhiên, cách làm này lại làm tăng yêu cầu lưu trữ mà chúng ta đang muốn giảm đi. Do đó chúng ta mới gọi thuật toán này là BetterBWTMatching. Một cách cân đối giữa runtime và storage là chỉ lưu một phần cột Count này. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Chúng ta sẽ chỉ lưu trữ mảng Count (Count arrays) khi i chia hết cho C, trong đó C là hằng số; những mảng này được gọi là checkpoint arrays. Khi C lớn (trên thực tế C thường bằng 100) và bảng chữ cái là nhỏ (ví dụ: bốn nucleotide), check point arrays yêu cầu chỉ một phần bộ nhớ được sử dụng bởi BWT (Text).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Còn thời gian chạy thì sao? Sử dụng checkpoint arrays, chúng ta có thể tính toán con trỏ top và bottom trong một số bước không đổi. Vì mỗi chuỗi Pattern yêu cầu nhiều nhất |Pattern| cập nhật con trỏ, thuật toán BetterBWTMatching đã sửa đổi hiện yêu cầu thời gian chạy $\\mathcal{O} (|Patterns|)$, giống như sử dụng Trie hoặc Suffix Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bwt:  20001\n",
      "Number of patterns:  3915\n",
      "Number of substring matches of the i-th member of patterns in text: \n",
      "--- 2.589503049850464 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Length of bwt: ', len(bwt))\n",
    "print('Number of patterns: ', len(patterns))\n",
    "print('Number of substring matches of the i-th member of patterns in text: ')\n",
    "start_time = time.time()\n",
    "ki = InverseBurrowsWheelerTransform(bwt).match_multi_patterns_better_bwt_with_check_points(patterns, 500)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Locating Matched Patterns with BWT<a name = 'locate-bwt'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 Partial Suffix Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định vị trí khớp Pattern, một lần nữa chúng ta có thể sử dụng Suffix Array. Trong phần 2.4.1, Suffix Arrays ngay lập tức tìm thấy ba vị trí của \"ana\" trong \"panamabanana $\".\n",
    "Suffix Arrays làm cho công việc của chúng ta dễ dàng, nhưng nhớ lại rằng động lực ban đầu của chúng ta là sử dụng biến đổi Burrows-Wheeler là để giảm yêu cầu về bộ nhớ được sử dụng bởi Suffix Array cho việc khớp Pattern. Nếu chúng ta thêm Suffix Array vào thuật toán khớp Pattern dựa trên Burrows-Wheeler, thì chúng ta sẽ trở lại ngay nơi bắt đầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi sẽ xây dựng một Partial Suffixx Array của Text, ký hiệu là SuffixArray (Text, k), chỉ chứa các giá trị là bội số của một số nguyên dương k. Trong các thực tế, Partial Suffix Array thường được xây dựng cho k = 100, do đó giảm sử dụng bộ nhớ so với Suffix Array đầy đủ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong Partial Suffix Array ta chỉ lưu các dãy cách nhau k đơn vị. Do đó, khi tìm được một Pattern khớp với Text là \"ana\", ta sẽ dừng việc quay lui nếu tương ứng với hàng chứa Pattern = \"ana\" tồn tại dãy hậu tố một phần. Ngược lại, nếu chưa có Partial Suffix Array tương ứng ta tiếp tục quay lui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/Qp9cVjR.jpg\" width=\"800\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_partial_suffix_arrays(text, k):\n",
    "\n",
    "    text_patterns = []\n",
    "    partial_index = []\n",
    "    partial_array = []\n",
    "    l = len(text)\n",
    "    # Append all suffix array\n",
    "    for i in range(l):\n",
    "        text_patterns.append(text[i::])\n",
    "    # Find index of suffix array after sorting\n",
    "    suffix_array = np.argsort(text_patterns).tolist()\n",
    "\n",
    "    array_index = np.arange(0, l, 1)\n",
    "    frac = int((l-1)/k)\n",
    "    \n",
    "    for c in range(0, frac + 1):\n",
    "        # The distance between the positions of the suffix is k.\n",
    "        partial_index.append(array_index[suffix_array.index(c*k)])\n",
    "        partial_array.append(c*k)\n",
    "    # Sorting tuple of suffix array in order of partial index.\n",
    "    tuple_suffix_array = sorted(tuple(zip(partial_index, partial_array)))\n",
    "    \n",
    "    partial_suffix_array = dict(tuple_suffix_array)\n",
    "    suffix_array_index = list(partial_suffix_array.keys())\n",
    "\n",
    "    return(partial_suffix_array, suffix_array_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Comparing Runtime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta thử so sánh thời gian chạy của thuật toán xác định vị trí khớp của Pattern trong Text với các hệ số k khác nhau của Suffix Array. Với k = 1, tất cả các Partial Suffix Array trở thành Suffix Array, khi đó ta không phải quay lui bất kì Pattern nào sau khi đã khớp với Text, thời gian chạy rõ ràng là nhanh nhất nhưng yêu cầu về bộ nhớ sẽ cao nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bwt:  8241\n",
      "Number of patterns:  15134\n",
      "--- 3.6338999271392822 seconds ---\n"
     ]
    }
   ],
   "source": [
    "with open('D:/Data Science/Data/Mutations/How Do We Locate Disease/Burrows and Wheeler Set Up Checkpoints/dataset_303_4 (5).txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    f.close\n",
    "text = lines[0] + '$'\n",
    "patterns = lines[1::]\n",
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 1)\n",
    "bwt = get_bwt(text)\n",
    "\n",
    "import time\n",
    "print('Length of bwt: ', len(bwt))\n",
    "print('Number of patterns: ', len(patterns))\n",
    "start_time = time.time()\n",
    "positions = InverseBurrowsWheelerTransform(bwt).locate_multi_patterns_better_bwt(patterns, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với k = 100, thời gian chạy sẽ lâu hơn, ngược lại yêu cầu về bộ nhớ lại thấp hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.758271217346191 seconds ---\n"
     ]
    }
   ],
   "source": [
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 100)\n",
    "start_time = time.time()\n",
    "positions = InverseBurrowsWheelerTransform(bwt).locate_multi_patterns_better_bwt(patterns, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All starting positions in Text where a string from Patterns appears as a substring: \n",
      "['None', 'None', 'None', 'None', 'None', [5074], 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', [2344], 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', [2188], 'None', 'None', 'None', 'None', [1008], 'None', 'None', [3115], 'None', 'None', 'None', 'None', [2508, 7391], 'None', [2796, 2803, 5808], 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', [2197], 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None'] ...\n"
     ]
    }
   ],
   "source": [
    "print('All starting positions in Text where a string from Patterns appears as a substring: ')\n",
    "print(positions[0:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11, 12, 19, 20, 21, 22, 23, 24, 25, 26, 27, 36, 59, 60, 61, 62, 63, 64, 65, 66, 75, 76, 77, 78, 81, 87, 88, 97, 98, 101, 108, 124, 125, 130, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 157, 166, 167, 168, 169, 170, 171, 172, 173, 185, 193, 202, 203, 204, 205, 206, 207, 208, 209, 217, 224, 225, 226, 235, 237, 238, 239, 240, 241, 242, 252, 265, 269, 272, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 321, 323, 324, 325, 340, 341, 352, 353, 354, 355, 356, 357, 358, 359, 373, 374, 382, 383, 384, 394, 395, 409, 410, 422, 423, 424, 425, 426, 428, 429, 430, 448, 451, 460, 469, 470, 472, 473, 474, 475, 476, 482, 493, 494, 495, 496, 497, 499, 500, 510, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 538, 539, 543, 548, 549, 552, 553, 562, 571, 572, 573, 574, 575, 576, 577, 593, 594, 595, 596, 597, 598, 608, 609, 610, 611, 612, 613, 614, 615, 632, 642, 647, 652, 664, 665, 666, 667, 668, 669, 670, 671, 686, 688, 689, 690, 691, 692, 700, 701, 702, 711, 712, 713, 714, 715, 716, 717, 718, 719, 728, 729, 737, 746, 747, 757, 758, 762, 767, 785, 786, 800, 801, 803, 804, 805, 806, 807, 827, 828, 829, 851, 852, 853, 861, 862, 863, 865, 866, 867, 868, 878, 879, 880, 901, 902, 912, 913, 919, 929, 930, 938, 939, 940, 950, 952, 953, 954, 955, 956, 959, 962, 965, 966, 968, 969, 977, 978, 979, 980, 981, 982, 992, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1014, 1018, 1027, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1058, 1064, 1065, 1066, 1073, 1074, 1077, 1078, 1083, 1084, 1085, 1086, 1089, 1092, 1106, 1107, 1122, 1123, 1124, 1126, 1127, 1138, 1139, 1151, 1155, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1178, 1183, 1187, 1196, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1227, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1245, 1255, 1262, 1263, 1264, 1279, 1280, 1281, 1282, 1283, 1284, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1310, 1314, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1336, 1337, 1338, 1339, 1340, 1341, 1343, 1352, 1353, 1354, 1355, 1356, 1359, 1365, 1368, 1369, 1370, 1371, 1381, 1382, 1386, 1396, 1397, 1398, 1399, 1400, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1420, 1431, 1440, 1441, 1443, 1444, 1445, 1446, 1447, 1456, 1474, 1492, 1505, 1506, 1507, 1508, 1522, 1523, 1524, 1525, 1526, 1527, 1529, 1530, 1537, 1538, 1551, 1552, 1554, 1555, 1556, 1557, 1558, 1569, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1600, 1613, 1614, 1615, 1625, 1626, 1635, 1636, 1642, 1649, 1654, 1655, 1664, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1692, 1693, 1694, 1708, 1709, 1710, 1711, 1713, 1714, 1715, 1724, 1733, 1735, 1736, 1737, 1738, 1739, 1740, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1775, 1776, 1777, 1778, 1783, 1790, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1830, 1841, 1842, 1843, 1845, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1873, 1874, 1875, 1876, 1884, 1892, 1893, 1895, 1903, 1904, 1919, 1920, 1922, 1923, 1924, 1939, 1948, 1957, 1958, 1959, 1960, 1961, 1970, 1971, 1976, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1993, 1995, 1996, 1997, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2021, 2022, 2023, 2032, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2069, 2073, 2074, 2085, 2092, 2098, 2099, 2100, 2101, 2102, 2104, 2105, 2106, 2107, 2108, 2120, 2123, 2155, 2156, 2169, 2170, 2171, 2172, 2173, 2185, 2188, 2191, 2197, 2202, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2253, 2262, 2263, 2269, 2274, 2290, 2314, 2315, 2330, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2353, 2356, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2383, 2391, 2392, 2393, 2411, 2416, 2430, 2431, 2433, 2434, 2435, 2436, 2437, 2438, 2444, 2447, 2459, 2461, 2462, 2464, 2465, 2466, 2476, 2487, 2488, 2491, 2495, 2496, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2524, 2530, 2531, 2540, 2558, 2581, 2584, 2585, 2590, 2594, 2595, 2599, 2605, 2631, 2651, 2652, 2653, 2654, 2659, 2660, 2661, 2673, 2674, 2675, 2677, 2678, 2679, 2680, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2723, 2728, 2735, 2736, 2737, 2750, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2792, 2793, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2811, 2812, 2822, 2823, 2836, 2837, 2843, 2844, 2845, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2867, 2868, 2883, 2884, 2885, 2886, 2887, 2888, 2890, 2902, 2921, 2922, 2923, 2924, 2925, 2926, 2928, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2955, 2973, 2974, 2975, 2976, 2977, 2984, 2994, 2995, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3016, 3017, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3062, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3104, 3105, 3106, 3107, 3108, 3115, 3119, 3125, 3139, 3143, 3144, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3169, 3170, 3171, 3184, 3197, 3202, 3206, 3209, 3216, 3217, 3218, 3219, 3230, 3244, 3249, 3258, 3259, 3263, 3266, 3267, 3279, 3280, 3281, 3283, 3284, 3285, 3286, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3313, 3322, 3334, 3335, 3336, 3347, 3356, 3357, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3392, 3393, 3411, 3412, 3414, 3415, 3416, 3417, 3418, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3482, 3484, 3485, 3487, 3488, 3489, 3505, 3512, 3527, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3543, 3551, 3555, 3556, 3576, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3599, 3601, 3602, 3603, 3604, 3613, 3614, 3616, 3617, 3618, 3619, 3620, 3621, 3631, 3632, 3633, 3634, 3635, 3636, 3645, 3646, 3647, 3648, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3674, 3675, 3676, 3677, 3678, 3679, 3682, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3714, 3726, 3727, 3730, 3739, 3756, 3767, 3768, 3769, 3770, 3778, 3788, 3802, 3811, 3826, 3829, 3835, 3836, 3837, 3842, 3843, 3848, 3849, 3854, 3860, 3861, 3870, 3871, 3876, 3887, 3890, 3891, 3892, 3893, 3907, 3908, 3909, 3910, 3911, 3912, 3914, 3925, 3929, 3930, 3931, 3932, 3933, 3934, 3936, 3945, 3946, 3947, 3956, 3957, 3958, 3959, 3960, 3961, 3979, 3980, 3983, 3988, 3989, 3998, 3999, 4011, 4020, 4021, 4022, 4029, 4030, 4040, 4043, 4044, 4045, 4053, 4054, 4055, 4056, 4058, 4059, 4060, 4061, 4075, 4076, 4078, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4101, 4119, 4120, 4129, 4130, 4132, 4133, 4134, 4135, 4136, 4137, 4152, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4179, 4211, 4212, 4213, 4214, 4215, 4218, 4219, 4220, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4244, 4250, 4258, 4259, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4298, 4307, 4308, 4309, 4310, 4311, 4312, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4331, 4338, 4339, 4343, 4351, 4360, 4385, 4386, 4387, 4389, 4390, 4391, 4392, 4393, 4398, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4434, 4435, 4453, 4455, 4456, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4491, 4492, 4493, 4495, 4496, 4497, 4498, 4499, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4536, 4558, 4559, 4560, 4561, 4562, 4571, 4578, 4591, 4592, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4607, 4608, 4614, 4619, 4628, 4629, 4630, 4632, 4633, 4634, 4635, 4636, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4677, 4681, 4691, 4693, 4694, 4695, 4696, 4697, 4698, 4715, 4716, 4728, 4729, 4730, 4731, 4733, 4734, 4735, 4749, 4750, 4751, 4759, 4760, 4768, 4769, 4770, 4771, 4772, 4773, 4782, 4785, 4794, 4796, 4797, 4798, 4799, 4820, 4821, 4839, 4840, 4847, 4852, 4856, 4876, 4877, 4878, 4879, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4914, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4943, 4953, 4962, 4963, 4964, 4965, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4994, 4995, 4996, 4997, 4999, 5001, 5004, 5011, 5012, 5020, 5027, 5028, 5029, 5037, 5054, 5060, 5065, 5066, 5067, 5074, 5078, 5087, 5092, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5118, 5134, 5137, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5191, 5192, 5194, 5195, 5209, 5210, 5219, 5222, 5223, 5229, 5232, 5251, 5252, 5255, 5258, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5288, 5293, 5294, 5295, 5296, 5297, 5316, 5333, 5342, 5351, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5426, 5429, 5445, 5446, 5447, 5448, 5449, 5451, 5452, 5453, 5454, 5463, 5464, 5465, 5466, 5467, 5468, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5500, 5501, 5509, 5510, 5511, 5512, 5513, 5528, 5538, 5539, 5540, 5541, 5542, 5543, 5545, 5546, 5558, 5559, 5563, 5583, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5611, 5615, 5616, 5617, 5624, 5627, 5634, 5638, 5643, 5663, 5665, 5666, 5667, 5668, 5670, 5680, 5681, 5682, 5683, 5692, 5719, 5720, 5721, 5722, 5723, 5724, 5726, 5727, 5732, 5735, 5736, 5748, 5761, 5768, 5774, 5775, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5832, 5833, 5835, 5836, 5838, 5839, 5840, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5875, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5915, 5923, 5924, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5959, 5960, 5961, 5962, 5967, 5970, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5999, 6007, 6026, 6029, 6038, 6039, 6040, 6049, 6066, 6067, 6084, 6100, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6122, 6123, 6125, 6126, 6127, 6128, 6129, 6130, 6139, 6140, 6152, 6157, 6174, 6179, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6210, 6211, 6216, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6265, 6267, 6268, 6278, 6300, 6316, 6325, 6326, 6328, 6332, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6360, 6365, 6370, 6377, 6378, 6381, 6382, 6383, 6384, 6386, 6388, 6392, 6398, 6399, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6424, 6435, 6447, 6449, 6450, 6451, 6452, 6453, 6454, 6467, 6468, 6479, 6480, 6488, 6490, 6491, 6493, 6494, 6495, 6503, 6504, 6505, 6514, 6520, 6526, 6530, 6531, 6532, 6533, 6547, 6558, 6559, 6560, 6561, 6572, 6578, 6579, 6580, 6581, 6582, 6583, 6585, 6602, 6603, 6612, 6625, 6626, 6627, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6675, 6676, 6679, 6684, 6685, 6688, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6715, 6720, 6721, 6730, 6747, 6748, 6749, 6750, 6751, 6752, 6762, 6765, 6766, 6767, 6768, 6769, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6801, 6802, 6810, 6815, 6817, 6818, 6819, 6820, 6822, 6840, 6841, 6842, 6843, 6844, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6863, 6864, 6882, 6890, 6899, 6905, 6921, 6922, 6939, 6940, 6945, 6946, 6947, 6948, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6983, 6988, 6990, 6991, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7076, 7077, 7078, 7080, 7081, 7082, 7083, 7084, 7101, 7102, 7105, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7139, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7167, 7168, 7197, 7225, 7226, 7231, 7241, 7242, 7243, 7246, 7263, 7264, 7268, 7279, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7353, 7363, 7365, 7374, 7382, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7401, 7402, 7419, 7420, 7421, 7422, 7423, 7433, 7444, 7445, 7446, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7513, 7514, 7516, 7517, 7518, 7519, 7520, 7529, 7530, 7539, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7568, 7584, 7593, 7597, 7601, 7602, 7603, 7604, 7616, 7617, 7624, 7641, 7642, 7654, 7659, 7665, 7668, 7669, 7670, 7672, 7673, 7674, 7675, 7676, 7677, 7686, 7690, 7694, 7702, 7703, 7704, 7706, 7707, 7708, 7709, 7710, 7720, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7734, 7738, 7739, 7750, 7756, 7777, 7781, 7787, 7788, 7789, 7790, 7791, 7795, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7849, 7860, 7861, 7868, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7894, 7898, 7903, 7912, 7923, 7924, 7925, 7926, 7928, 7929, 7930, 7942, 7943, 7953, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7978, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 8002, 8009, 8018, 8023, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8072, 8075, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8100, 8104, 8107, 8108, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8140, 8149, 8154, 8164, 8165, 8180, 8181, 8184, 8193, 8220, 8221, 8222, 8223, 8224, 8225, 8226]\n"
     ]
    }
   ],
   "source": [
    "while 'None' in positions:\n",
    "    positions.remove('None')\n",
    "result = []\n",
    "for x in positions:\n",
    "    result += x\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với thuật toán có sử dụng checkpoint arrays, runtime sẽ nhanh hơn với một hệ số C hợp lý, ở đây ta thử với C = 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.482427597045898 seconds ---\n"
     ]
    }
   ],
   "source": [
    "patterns = lines[1:-1]\n",
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 50)\n",
    "start_time = time.time()\n",
    "positions = InverseBurrowsWheelerTransform(bwt).locate_multi_patterns_better_bwt_with_check_points(patterns, 50, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta chỉ phải lưu trữ những dữ liệu sau vào bộ nhớ: BWT (Text), FirstOccurence, Partial Suffix Array và Checkpoint Array. Lưu trữ dữ liệu này yêu cầu bộ nhớ xấp xỉ bằng 1,5 · |Text|. Vì vậy, cuối cùng chúng ta đã giảm được bộ nhớ cần thiết để giải quyết bài toán Multiple Pattern Matching cho hàng triệu sequencing reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Approximate pattern matching with the Burrows-Wheeler transform <a name = 'approximate'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể biết đến Basic Local Alignment Search Tool (BLAST) dựa trên thuật toán tham lam có thể tìm thấy sự tương đồng giữa các protein ngay cả khi tất cả các axit amin trong một protein đã bị đột biến so với protein khác [16]. Tuy nhiên ở đây, chúng ta sẽ tập trung vào phương pháp liên quan đến Burrows-Wheeler Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để mở rộng Burrows-Wheeler Transform cho khớp Pattern gần đúng, chúng ta sẽ không dừng lại khi gặp phải một mismatch. Ngược lại, chúng ta sẽ tiếp tục cho đến khi hoặc tìm thấy một kết quả gần đúng hoặc vượt quá giới hạn d mismatch [17]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình dưới đây minh họa việc tìm kiếm \"asa\" trong \"panamabananas$\" với nhiều nhất là 1\n",
    "mismatch. Trước tiên, hãy tiến hành như trong trường hợp khớp mẫu chính xác, xâu ngược chuỗi \"asa\"\n",
    "bằng cách sử dụng biến đổi Burrows-Wheeler. Sau khi tìm thấy sáu lần xuất hiện của \"a\",\n",
    "chúng ta xác định sáu lần xuất hiện không chính xác của \"sa\" là: \"pa\", \"ma\", \"ba\" và ba lần xuất hiện của\"na\". Lưu ý rằng ba chuỗi này đã tích lũy một mismatch và sau đó tiếp tục\n",
    "qúa trình tương tự với tất cả sáu chuỗi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bước tiếp theo, năm trong số các lần xuất hiện mismatch của \"sa\" có thể được mở rộng thành số lần xuất hiện của \"asa\" chỉ với một mismatch duy nhất: \"ama\", \"aba\" và ba lần xuất hiện\n",
    "của \"ana\". Chúng ta không mở rộng \"pa\", mà loại bỏ vì mismatch vượt quá 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong thực tế, phương pháp tham lam này phải đối mặt với một số vấn đề phức tạp. Chúng ta không muốn giai đoạn đầu có các chuỗi không khớp, ngược lại chúng ta sẽ phải xem xét rất nhiều chuỗi không phù hợp. Do đó, chúng ta có thể chỉ xem xét các hậu tố của Pattern với một ngưỡng độ dài mà hậu tố đó khớp chính xác với Text. Hơn nữa, phương pháp này sẽ tiêu tốn nhiều thời gian khi giá trị của d lớn, vì chúng ta phải tìm kiếm nhiều khớp không chính xác. Trên thực tế, người ta thường giới hạn giá trị của d tối đa là 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://imgur.com/QadpjUW.jpg\" width=\"600\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of text:  10001\n",
      "Number of patterns:  2000\n"
     ]
    }
   ],
   "source": [
    "with open('D:/Data Science/Data/Mutations/How Do We Locate Disease/Burrows and Wheeler Set Up Checkpoints/dataset_304_10 (4).txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    f.close\n",
    "text = lines[0] + '$'\n",
    "patterns = lines[1:-1][0].split()\n",
    "d = int(lines[-1])\n",
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 1)\n",
    "bwt = get_bwt(text)\n",
    "print('The length of text: ', len(text))\n",
    "print('Number of patterns: ', len(patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 50.273003816604614 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "starting_positions = InverseBurrowsWheelerTransform(bwt).get_approximate_pattern_matching_with_better_bwt(patterns, d, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 53.94083285331726 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "c = 100\n",
    "start_time = time.time()\n",
    "starting_positions = InverseBurrowsWheelerTransform(bwt).get_approximate_pattern_matching_with_check_points(patterns, c, d, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 9, 11, 13, 15, 21, 27, 31, 40, 40, 44, 50, 54, 54, 57, 61, 62, 69, 76, 95, 100, 102, 103, 106, 107, 119, 124, 129, 139, 145, 157, 186, 192, 192, 195, 197, 200, 212, 223, 223, 224, 229, 243, 244, 245, 261, 264, 270, 274, 287, 301, 304, 304, 306, 312, 313, 323, 327, 329, 330, 340, 346, 357, 359, 359, 359, 363, 364, 372, 379, 386, 388, 394, 403, 404, 409, 420, 428, 440, 441, 453, 458, 459, 460, 461, 462, 465, 473, 481, 485, 497, 509, 513, 513, 527, 530, 535, 536, 537, 539, 541, 547, 547, 553, 557, 562, 566, 568, 573, 582, 592, 593, 598, 601, 603, 607, 635, 640, 649, 652, 660, 661, 667, 671, 682, 696, 708, 709, 722, 726, 734, 740, 740, 746, 747, 764, 765, 772, 779, 781, 784, 789, 790, 803, 809, 821, 837, 839, 841, 844, 845, 849, 849, 850, 853, 853, 858, 867, 869, 882, 887, 902, 907, 910, 911, 912, 919, 919, 927, 929, 931, 935, 940, 941, 948, 953, 960, 960, 966, 980, 981, 988, 990, 991, 993, 994, 996, 996, 1002, 1005, 1008, 1021, 1022, 1023, 1023, 1024, 1032, 1033, 1050, 1056, 1061, 1077, 1083, 1089, 1101, 1103, 1104, 1105, 1113, 1116, 1126, 1131, 1139, 1147, 1149, 1169, 1176, 1179, 1180, 1185, 1185, 1187, 1188, 1193, 1199, 1200, 1203, 1206, 1209, 1213, 1214, 1216, 1219, 1226, 1231, 1234, 1242, 1255, 1261, 1267, 1268, 1270, 1278, 1283, 1285, 1287, 1289, 1290, 1290, 1294, 1306, 1306, 1307, 1314, 1325, 1327, 1327, 1329, 1342, 1349, 1354, 1359, 1360, 1363, 1371, 1376, 1378, 1386, 1396, 1403, 1408, 1426, 1431, 1434, 1435, 1438, 1444, 1453, 1459, 1467, 1470, 1470, 1471, 1472, 1477, 1483, 1492, 1492, 1499, 1499, 1501, 1507, 1513, 1522, 1528, 1533, 1533, 1534, 1535, 1547, 1547, 1548, 1567, 1568, 1568, 1569, 1572, 1580, 1584, 1594, 1595, 1597, 1602, 1609, 1624, 1631, 1632, 1638, 1645, 1656, 1663, 1675, 1675, 1676, 1680, 1685, 1694, 1695, 1697, 1708, 1711, 1715, 1723, 1723, 1724, 1728, 1730, 1732, 1732, 1734, 1734, 1746, 1749, 1758, 1760, 1762, 1775, 1780, 1797, 1800, 1804, 1815, 1817, 1823, 1836, 1838, 1838, 1841, 1843, 1846, 1858, 1859, 1867, 1875, 1886, 1897, 1898, 1899, 1913, 1915, 1923, 1924, 1924, 1927, 1932, 1933, 1933, 1934, 1945, 1948, 1949, 1951, 1972, 1972, 1976, 1979, 1982, 1983, 1986, 1988, 1990, 2006, 2007, 2008, 2009, 2018, 2023, 2026, 2030, 2039, 2044, 2047, 2048, 2080, 2083, 2089, 2090, 2091, 2094, 2097, 2102, 2116, 2117, 2122, 2123, 2128, 2132, 2139, 2156, 2161, 2163, 2165, 2172, 2178, 2180, 2184, 2187, 2191, 2200, 2201, 2203, 2225, 2232, 2237, 2238, 2249, 2254, 2267, 2271, 2271, 2274, 2277, 2280, 2280, 2281, 2284, 2289, 2297, 2307, 2313, 2325, 2328, 2332, 2337, 2339, 2342, 2345, 2359, 2364, 2366, 2368, 2369, 2373, 2375, 2378, 2378, 2406, 2413, 2413, 2416, 2421, 2434, 2436, 2443, 2447, 2449, 2450, 2463, 2469, 2489, 2496, 2501, 2504, 2519, 2531, 2542, 2560, 2563, 2567, 2568, 2573, 2581, 2591, 2600, 2601, 2615, 2626, 2631, 2634, 2642, 2643, 2646, 2647, 2660, 2666, 2670, 2677, 2679, 2680, 2685, 2686, 2690, 2697, 2697, 2698, 2724, 2728, 2729, 2738, 2743, 2746, 2748, 2753, 2755, 2757, 2761, 2764, 2767, 2768, 2780, 2781, 2784, 2788, 2797, 2798, 2799, 2810, 2810, 2812, 2814, 2819, 2824, 2842, 2845, 2848, 2855, 2856, 2868, 2874, 2880, 2887, 2890, 2892, 2897, 2899, 2907, 2929, 2929, 2931, 2934, 2935, 2942, 2955, 2969, 2974, 2982, 2982, 2984, 2989, 2989, 2990, 3000, 3001, 3014, 3019, 3019, 3020, 3024, 3034, 3037, 3038, 3040, 3047, 3054, 3057, 3058, 3075, 3076, 3078, 3079, 3092, 3095, 3097, 3099, 3101, 3106, 3121, 3123, 3125, 3132, 3134, 3134, 3143, 3144, 3146, 3147, 3151, 3153, 3159, 3161, 3161, 3167, 3168, 3170, 3171, 3181, 3183, 3185, 3186, 3186, 3187, 3190, 3192, 3202, 3203, 3209, 3215, 3217, 3221, 3223, 3236, 3243, 3245, 3248, 3248, 3250, 3255, 3258, 3267, 3268, 3282, 3287, 3293, 3297, 3300, 3306, 3310, 3313, 3323, 3325, 3330, 3332, 3334, 3338, 3339, 3339, 3342, 3343, 3345, 3351, 3357, 3358, 3359, 3362, 3372, 3373, 3375, 3376, 3380, 3385, 3387, 3388, 3396, 3396, 3400, 3400, 3402, 3407, 3407, 3413, 3414, 3424, 3432, 3435, 3445, 3447, 3452, 3465, 3476, 3481, 3488, 3490, 3490, 3494, 3498, 3500, 3502, 3502, 3521, 3532, 3539, 3545, 3550, 3551, 3553, 3557, 3559, 3564, 3583, 3596, 3601, 3605, 3606, 3607, 3611, 3614, 3623, 3626, 3627, 3633, 3639, 3639, 3640, 3655, 3656, 3665, 3675, 3677, 3697, 3697, 3721, 3729, 3738, 3763, 3769, 3781, 3787, 3787, 3799, 3803, 3812, 3817, 3818, 3823, 3824, 3828, 3838, 3853, 3860, 3861, 3869, 3873, 3875, 3875, 3879, 3885, 3887, 3888, 3889, 3897, 3905, 3906, 3908, 3908, 3921, 3924, 3924, 3924, 3926, 3926, 3936, 3937, 3941, 3945, 3949, 3957, 3961, 3963, 3963, 3971, 3975, 3990, 3995, 3995, 4002, 4006, 4009, 4019, 4022, 4026, 4027, 4038, 4041, 4041, 4047, 4049, 4055, 4055, 4056, 4059, 4067, 4070, 4070, 4071, 4076, 4088, 4094, 4108, 4127, 4141, 4144, 4145, 4153, 4154, 4156, 4157, 4161, 4166, 4169, 4173, 4175, 4176, 4176, 4184, 4195, 4198, 4202, 4206, 4212, 4213, 4220, 4223, 4232, 4233, 4235, 4238, 4239, 4242, 4250, 4251, 4255, 4255, 4263, 4284, 4286, 4292, 4292, 4295, 4296, 4302, 4310, 4313, 4320, 4328, 4328, 4338, 4351, 4362, 4363, 4367, 4368, 4373, 4382, 4383, 4386, 4386, 4393, 4394, 4406, 4435, 4437, 4437, 4438, 4439, 4441, 4451, 4452, 4452, 4458, 4462, 4464, 4483, 4499, 4503, 4504, 4517, 4519, 4520, 4532, 4544, 4544, 4552, 4556, 4559, 4559, 4562, 4569, 4574, 4582, 4591, 4595, 4599, 4604, 4612, 4622, 4628, 4632, 4641, 4643, 4660, 4661, 4670, 4673, 4679, 4685, 4688, 4692, 4701, 4707, 4708, 4708, 4726, 4727, 4727, 4743, 4745, 4745, 4746, 4752, 4755, 4758, 4759, 4760, 4764, 4764, 4765, 4767, 4769, 4771, 4777, 4777, 4782, 4803, 4814, 4819, 4824, 4826, 4831, 4835, 4841, 4841, 4853, 4856, 4865, 4868, 4869, 4875, 4878, 4879, 4880, 4880, 4886, 4889, 4891, 4896, 4901, 4903, 4908, 4913, 4917, 4934, 4936, 4950, 4951, 4967, 4971, 4973, 4977, 4986, 4993, 4995, 5000, 5002, 5003, 5005, 5011, 5027, 5031, 5040, 5040, 5044, 5049, 5057, 5059, 5064, 5068, 5069, 5077, 5080, 5083, 5087, 5088, 5089, 5092, 5094, 5094, 5095, 5095, 5096, 5096, 5096, 5097, 5102, 5119, 5120, 5126, 5134, 5142, 5142, 5151, 5158, 5166, 5183, 5192, 5201, 5202, 5217, 5225, 5225, 5234, 5238, 5239, 5243, 5244, 5247, 5250, 5251, 5251, 5253, 5255, 5258, 5258, 5263, 5268, 5268, 5270, 5271, 5273, 5277, 5282, 5288, 5291, 5293, 5297, 5298, 5301, 5306, 5313, 5317, 5322, 5323, 5323, 5324, 5328, 5340, 5349, 5352, 5356, 5360, 5360, 5361, 5364, 5365, 5370, 5378, 5381, 5382, 5386, 5389, 5391, 5392, 5396, 5398, 5399, 5410, 5413, 5420, 5424, 5446, 5452, 5457, 5466, 5467, 5479, 5497, 5497, 5500, 5508, 5509, 5509, 5512, 5518, 5521, 5523, 5525, 5531, 5534, 5541, 5547, 5548, 5557, 5566, 5567, 5567, 5572, 5578, 5584, 5586, 5594, 5602, 5602, 5607, 5611, 5617, 5619, 5620, 5621, 5626, 5630, 5634, 5641, 5641, 5642, 5644, 5649, 5649, 5664, 5670, 5670, 5674, 5677, 5682, 5693, 5698, 5703, 5708, 5714, 5715, 5715, 5716, 5720, 5721, 5744, 5746, 5751, 5753, 5753, 5756, 5762, 5771, 5771, 5775, 5785, 5785, 5788, 5788, 5799, 5801, 5804, 5807, 5807, 5812, 5827, 5828, 5834, 5841, 5848, 5849, 5852, 5857, 5858, 5859, 5871, 5877, 5882, 5886, 5889, 5891, 5899, 5909, 5912, 5916, 5916, 5917, 5924, 5939, 5940, 5948, 5949, 5949, 5954, 5965, 5967, 5968, 5971, 5975, 5983, 5993, 5997, 6000, 6012, 6019, 6023, 6026, 6032, 6041, 6056, 6068, 6069, 6077, 6079, 6086, 6091, 6097, 6097, 6100, 6106, 6110, 6111, 6117, 6118, 6134, 6138, 6144, 6149, 6158, 6164, 6165, 6165, 6174, 6178, 6178, 6178, 6187, 6191, 6195, 6199, 6200, 6201, 6210, 6215, 6218, 6222, 6234, 6240, 6241, 6245, 6257, 6261, 6265, 6268, 6279, 6280, 6287, 6292, 6295, 6300, 6304, 6305, 6305, 6306, 6309, 6319, 6323, 6325, 6326, 6328, 6330, 6343, 6343, 6346, 6352, 6352, 6355, 6357, 6368, 6368, 6375, 6376, 6376, 6379, 6380, 6382, 6387, 6391, 6392, 6415, 6415, 6427, 6428, 6436, 6438, 6449, 6464, 6472, 6476, 6484, 6484, 6495, 6500, 6503, 6503, 6509, 6513, 6520, 6524, 6524, 6529, 6532, 6547, 6548, 6549, 6550, 6552, 6554, 6563, 6567, 6567, 6570, 6574, 6581, 6583, 6606, 6622, 6623, 6623, 6626, 6628, 6633, 6638, 6639, 6647, 6650, 6655, 6656, 6659, 6659, 6665, 6670, 6670, 6676, 6677, 6679, 6688, 6694, 6699, 6700, 6703, 6704, 6709, 6734, 6748, 6754, 6760, 6781, 6807, 6809, 6810, 6812, 6819, 6820, 6823, 6835, 6838, 6841, 6848, 6849, 6858, 6859, 6866, 6867, 6871, 6872, 6874, 6885, 6886, 6888, 6919, 6920, 6938, 6942, 6946, 6955, 6957, 6958, 6958, 6962, 6962, 6969, 6978, 6978, 6990, 7000, 7014, 7022, 7037, 7038, 7043, 7052, 7054, 7064, 7073, 7081, 7086, 7088, 7092, 7094, 7101, 7109, 7111, 7118, 7129, 7130, 7132, 7135, 7136, 7140, 7167, 7172, 7188, 7195, 7224, 7233, 7234, 7239, 7244, 7246, 7247, 7248, 7248, 7249, 7254, 7255, 7256, 7267, 7270, 7275, 7280, 7284, 7285, 7293, 7297, 7306, 7309, 7312, 7318, 7322, 7327, 7330, 7341, 7352, 7355, 7363, 7366, 7366, 7366, 7367, 7370, 7377, 7379, 7380, 7388, 7391, 7392, 7392, 7393, 7397, 7403, 7403, 7414, 7416, 7418, 7421, 7423, 7427, 7431, 7448, 7449, 7455, 7455, 7459, 7461, 7465, 7476, 7479, 7483, 7485, 7488, 7497, 7499, 7513, 7513, 7517, 7518, 7520, 7520, 7532, 7532, 7533, 7549, 7552, 7558, 7564, 7567, 7581, 7586, 7589, 7591, 7597, 7601, 7607, 7619, 7624, 7625, 7625, 7627, 7632, 7652, 7659, 7666, 7667, 7697, 7701, 7702, 7702, 7707, 7707, 7710, 7714, 7716, 7717, 7721, 7724, 7742, 7743, 7745, 7755, 7768, 7771, 7772, 7774, 7784, 7787, 7789, 7792, 7794, 7797, 7798, 7816, 7823, 7824, 7826, 7833, 7834, 7836, 7841, 7846, 7846, 7855, 7863, 7864, 7865, 7873, 7874, 7878, 7885, 7888, 7888, 7892, 7894, 7895, 7898, 7904, 7905, 7912, 7915, 7925, 7931, 7933, 7939, 7940, 7942, 7944, 7945, 7947, 7948, 7948, 7961, 7963, 7964, 7973, 7985, 7988, 7993, 7996, 7997, 8000, 8009, 8018, 8018, 8025, 8032, 8038, 8038, 8039, 8041, 8047, 8050, 8071, 8077, 8086, 8091, 8094, 8096, 8107, 8113, 8113, 8114, 8115, 8115, 8116, 8119, 8120, 8123, 8124, 8126, 8132, 8137, 8141, 8143, 8147, 8154, 8166, 8173, 8174, 8175, 8179, 8181, 8193, 8197, 8204, 8206, 8208, 8220, 8228, 8241, 8242, 8243, 8244, 8256, 8265, 8273, 8273, 8277, 8290, 8294, 8300, 8300, 8300, 8305, 8307, 8309, 8311, 8311, 8316, 8316, 8319, 8322, 8327, 8331, 8335, 8343, 8353, 8366, 8366, 8387, 8397, 8400, 8405, 8411, 8415, 8417, 8421, 8424, 8438, 8448, 8449, 8452, 8458, 8467, 8469, 8470, 8476, 8488, 8494, 8496, 8498, 8507, 8510, 8520, 8533, 8536, 8542, 8545, 8546, 8546, 8551, 8552, 8554, 8555, 8560, 8563, 8565, 8570, 8572, 8573, 8576, 8579, 8580, 8581, 8589, 8594, 8596, 8598, 8613, 8614, 8614, 8616, 8621, 8637, 8637, 8642, 8643, 8648, 8650, 8657, 8664, 8678, 8679, 8679, 8685, 8685, 8687, 8706, 8718, 8722, 8726, 8727, 8727, 8729, 8730, 8730, 8731, 8739, 8740, 8750, 8753, 8767, 8768, 8773, 8774, 8789, 8794, 8797, 8808, 8826, 8829, 8831, 8833, 8844, 8847, 8855, 8856, 8861, 8861, 8864, 8865, 8876, 8883, 8884, 8896, 8896, 8896, 8901, 8904, 8910, 8913, 8931, 8931, 8932, 8941, 8948, 8952, 8954, 8965, 8967, 8969, 8970, 8972, 8975, 8986, 8987, 8988, 8997, 9011, 9011, 9036, 9037, 9041, 9044, 9046, 9066, 9066, 9068, 9073, 9090, 9091, 9092, 9093, 9093, 9094, 9095, 9107, 9112, 9119, 9128, 9132, 9135, 9144, 9145, 9147, 9152, 9153, 9156, 9161, 9170, 9171, 9175, 9176, 9190, 9195, 9195, 9197, 9209, 9230, 9235, 9243, 9246, 9255, 9256, 9261, 9263, 9265, 9268, 9280, 9281, 9282, 9288, 9289, 9293, 9299, 9308, 9315, 9318, 9319, 9326, 9329, 9333, 9333, 9343, 9344, 9346, 9351, 9352, 9353, 9364, 9369, 9371, 9372, 9389, 9394, 9404, 9404, 9406, 9416, 9427, 9431, 9432, 9435, 9438, 9442, 9452, 9458, 9461, 9477, 9479, 9480, 9482, 9484, 9484, 9494, 9495, 9496, 9497, 9506, 9518, 9520, 9530, 9530, 9531, 9534, 9562, 9563, 9570, 9575, 9590, 9597, 9598, 9605, 9608, 9614, 9625, 9626, 9630, 9652, 9659, 9664, 9666, 9670, 9673, 9676, 9677, 9681, 9684, 9686, 9687, 9695, 9698, 9702, 9702, 9710, 9712, 9714, 9717, 9723, 9725, 9728, 9759, 9768, 9769, 9776, 9776, 9782, 9782, 9784, 9790, 9799, 9801, 9810, 9819, 9821, 9823, 9824, 9825, 9830, 9833, 9835, 9852, 9860, 9866, 9868, 9878, 9880, 9884, 9884, 9887, 9887, 9893, 9894, 9902, 9903, 9904, 9905, 9910, 9912, 9932, 9935, 9939]\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "for x in starting_positions:\n",
    "    for el in x:\n",
    "        start_positions.append(el)\n",
    "\n",
    "print(sorted(start_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography: <br>\n",
    "[15] Burrows, M., and D. J. Wheeler. “A Block-Sorting Lossless Data Compression Algorithm,” 1994.<br>\n",
    "[16] Altschul, S. F., W. Gish, W. Miller, E. W. Myers, and D. J. Lipman. “Basic Local Alignment Search Tool.” <i>Journal of Molecular Biology</i> 215, no. 3 (October 5, 1990): 403–10. https://doi.org/10.1016/S0022-2836(05)80360-2.<br>\n",
    "[17] An efficient implementation of the Burrows-Wheeler transform was described by Ferragina and Manzini, 2000<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseBurrowsWheelerTransformNoneRecurrent(object):\n",
    "    \n",
    "    def __init__(self, bwt):\n",
    "        self._bwt = bwt\n",
    "        # Starting Positions of Patterns matching Text.\n",
    "        self._start_positions = []\n",
    "        # Generate first column from BWT.\n",
    "        self._first_column = sorted(self._bwt)\n",
    "        # Keeping last column in string type to speed up runtime.\n",
    "        self._last_column_str = self._bwt\n",
    "        # Last column in list type.\n",
    "        self._last_column = list(self._bwt)\n",
    "        # Get symbols of the first column.\n",
    "        self._alphabet = np.unique(self._first_column)\n",
    "        \n",
    "        # Adding index to the first column.\n",
    "        self._index_first_column = self.index_symbol_of_column(self._first_column)\n",
    "        # Adding the same index of corresponding symbols to the last column.\n",
    "        self._index_last_column = self.index_symbol_of_column(self._last_column)\n",
    "        # Get index of the first symbol of the first column.\n",
    "        self._first_occurence = self.get_first_occurence()\n",
    "        \n",
    "    # Inverting a bwt to original text\n",
    "    def invert_bwt(self):\n",
    "        \n",
    "        org_string = []\n",
    "        length = len(self._index_first_column)\n",
    "        \n",
    "        # Find row concluding symbol (0, '$') at last.\n",
    "        index_str = self._index_last_column.index((0, '$'))\n",
    "        \n",
    "        while 1 < 2:\n",
    "            # Finding the first symbol of the row above\n",
    "            org_symbol_of_string = self._index_first_column[index_str]\n",
    "            # Only choosing symbol, excluding index.\n",
    "            org_string.append(org_symbol_of_string[1])\n",
    "            # The loop is going on.\n",
    "            index_str = self._index_last_column.index(org_symbol_of_string)\n",
    "            # If the loop returns symbol (0, '$'), we stop the loop.\n",
    "            if org_symbol_of_string == (0, '$'):\n",
    "                break\n",
    "                \n",
    "        return(''.join(org_string))\n",
    "\n",
    "    # Get approximatly patterns matching text by better bwt.\n",
    "    def get_approximate_pattern_matching_with_better_bwt(self, patterns, d, partial_suffix_array, suffix_array_index):\n",
    "        \n",
    "        starting_positions = []\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            \n",
    "            self._start_positions = []\n",
    "            # Set initial top and bottom.\n",
    "            top = 0\n",
    "            l = len(self._last_column)\n",
    "            bottom = l - 1\n",
    "            \n",
    "            \"\"\" If we want to assign the initial value of the pattern to match the symbol then run the hidden code folows:\"\"\"\n",
    "            #symbol = pattern[-1]\n",
    "            #pattern = pattern[0 : -1]\n",
    "\n",
    "            #if symbol in self._last_column[top : (bottom + 1)]:\n",
    "\n",
    "                #top_index = self._last_column[top : (bottom + 1)].index(symbol) + top\n",
    "                #bottom_index = bottom - list(reversed(self._last_column[top : (bottom + 1)])).index(symbol)\n",
    "                #top = self.get_last_to_first(top_index)\n",
    "                #bottom = self.get_last_to_first(bottom_index)\n",
    "                \n",
    "            starting_positions.append(self.get_sub_aproximate_string(pattern, top, bottom, d, 0, \n",
    "                                                                     partial_suffix_array, suffix_array_index)\n",
    "                                     )\n",
    "        return(starting_positions)\n",
    "    \n",
    "    \n",
    "    # Get approximatly patterns matching text by better bwt with checkpoints.\n",
    "    def get_approximate_pattern_matching_with_check_points(self, patterns, c, d, partial_suffix_array, suffix_array_index):\n",
    "        \n",
    "        starting_positions = []\n",
    "        self._index_check_point, self._count_dict = self.get_check_point_arrays(c)\n",
    "        l = len(self._last_column)\n",
    "      \n",
    "        for pattern in patterns:\n",
    "            self._start_positions = []\n",
    "            # Set initial top and bottom.\n",
    "            top = 0\n",
    "            bottom = l - 1\n",
    "            \n",
    "            \"\"\" If we want to assign the initial value of the pattern to match the symbol then run the hidden code folows:\"\"\"\n",
    "            #symbol = pattern[-1]\n",
    "            #pattern = pattern[0 : -1]\n",
    "\n",
    "            #if symbol in self._last_column[top : (bottom + 1)]:\n",
    "\n",
    "                #top_index = self._last_column[top : (bottom + 1)].index(symbol) + top\n",
    "                #bottom_index = bottom - list(reversed(self._last_column[top : (bottom + 1)])).index(symbol)\n",
    "                #top = self.get_last_to_first(top_index)\n",
    "                #bottom = self.get_last_to_first(bottom_index)\n",
    "                \n",
    "            starting_positions.append(self.get_sub_aproximate_string_with_check_points(pattern, top, bottom, d,\n",
    "                                                                                       partial_suffix_array, suffix_array_index)\n",
    "                                     )\n",
    "        return(starting_positions)\n",
    "    \n",
    "    \n",
    "    # Get approximatly pattern matching text by better bwt.\n",
    "    def get_sub_aproximate_string(self, pattern, top, bottom, d, initial_mismatch, \n",
    "                                  partial_suffix_array, suffix_array_index\n",
    "                                 ):\n",
    "        # Get symbol exiting from top to bottom of the last column.\n",
    "        apr_symbol_alphabet = ''.join(np.unique(self._last_column[top : (bottom + 1)]))\n",
    "        # If pattern equals '', we find the positions of text that pattern matches the text.\n",
    "        if pattern == '':\n",
    "            \n",
    "            for i in range(top, bottom + 1):\n",
    "                # Set the initial backward count.\n",
    "                count = 0\n",
    "                last_index = i\n",
    "                # When partial suffix array is not exist, we go backward again.\n",
    "                while last_index not in suffix_array_index:\n",
    "                    \n",
    "                    count += 1\n",
    "                    symbol = self._last_column_str[last_index]\n",
    "                    last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "            # Starting position of pattern equals sum of partial suffix array and extend backward count.        \n",
    "            self._start_positions.append(partial_suffix_array[last_index] + count)\n",
    "            \n",
    "        else:\n",
    "            symbol = pattern[-1] # Moving backward of Pattern.\n",
    "            pattern = pattern[:-1] # Remove the last symbol of pattern.\n",
    "            for apr_symbol in apr_symbol_alphabet:\n",
    "                # Set the initial mismatch.\n",
    "                mismatch = initial_mismatch\n",
    "                \n",
    "                if apr_symbol != symbol:\n",
    "                    mismatch += 1\n",
    "                # If mismatch <= d, we update top and bottom.\n",
    "                if mismatch <= d:   \n",
    "                    \n",
    "                    new_top = top\n",
    "                    new_bottom = bottom\n",
    "                    \n",
    "                    \"\"\" If we want to use normal bwt instead of better bwt then run the hidden code folows:\"\"\"\n",
    "                    #top_index = self._last_column[new_top : (new_bottom + 1)].index(apr_symbol) + new_top\n",
    "                    #bottom_index = new_bottom - list(reversed(self._last_column[new_top : (new_bottom + 1)])).index(apr_symbol)\n",
    "                    #new_top = self.get_last_to_first(top_index)\n",
    "                    #new_bottom = self.get_last_to_first(bottom_index)\n",
    "                    \n",
    "                    new_top = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_top)\n",
    "                    new_bottom = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_bottom + 1) - 1\n",
    "                    # Using recurence algorithm for the new patterns that are exact or in-exact matching text.\n",
    "                    self.get_sub_aproximate_string(pattern, new_top, new_bottom, d, \n",
    "                                                                    mismatch, partial_suffix_array, suffix_array_index\n",
    "                                                                    ) \n",
    "        return(self._start_positions)\n",
    "    \n",
    "    # Get approximatly pattern matching text by better bwt with checkpoints.\n",
    "    def get_sub_aproximate_string_with_check_points(self, pattern, top, bottom, d,\n",
    "                                                    partial_suffix_array, suffix_array_index\n",
    "                                                    ):\n",
    "        # Get symbol exiting from top to bottom of the last column.\n",
    "        # apr_symbol_alphabet = ''.join(np.unique(self._last_column[top : (bottom + 1)])) # f(x)\n",
    "        # If pattern equals '', we find the positions of text that pattern matches the text.\n",
    "        #self._index_check_point, self._count_dict = self.get_check_point_arrays(1)\n",
    "        # Set the initial mismatch.\n",
    "       \n",
    "        top_bottom_update = [[top, bottom, 0, 0], [1, 1, 1, -1]]\n",
    "        count = 0\n",
    "        while top_bottom_update != [[1, 1, 1, -1]]:\n",
    "            \n",
    "            t_bt = top_bottom_update[0]\n",
    "            \n",
    "            top = t_bt[0]\n",
    "            bottom = t_bt[1]\n",
    "            initial_mismatch = t_bt[2]\n",
    "            label = t_bt[3]\n",
    "            new_label = top_bottom_update[1][3]\n",
    "  \n",
    "            #print(top, bottom, initial_mismatch)\n",
    "            apr_symbol_alphabet = ''.join(np.unique(self._last_column[top : (bottom + 1)]))\n",
    "            \n",
    "            if pattern != '': # E(x)\n",
    "                top_bottom_update.pop(0)\n",
    "                symbol = pattern[-1]  # A(x)\n",
    "                \n",
    "                if label != new_label:\n",
    "                    pattern = pattern[:-1] # f(x)\n",
    "                \n",
    "                for apr_symbol in apr_symbol_alphabet:\n",
    "                    count = label + 1\n",
    "                    mismatch = initial_mismatch\n",
    "                    new_top = top\n",
    "                    new_bottom = bottom\n",
    "\n",
    "                    if apr_symbol != symbol:\n",
    "                        mismatch += 1 # f(x)\n",
    "                    #print(apr_symbol, symbol, mismatch)\n",
    "                    # If mismatch <= d, we update top and bottom.\n",
    "                    if mismatch <= d:   \n",
    "                        # Updating top and bottom using checkpoints.\n",
    "                        if new_top not in self._index_check_point:\n",
    "                            new_top = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_top)\n",
    "                        else:\n",
    "                            new_top = self._first_occurence[apr_symbol] + self._count_dict[new_top][apr_symbol]\n",
    "\n",
    "                        if new_bottom + 1 not in self._index_check_point:\n",
    "                            new_bottom = self._first_occurence[apr_symbol] + self.count_symbol(apr_symbol, new_bottom + 1) - 1 \n",
    "                        else:\n",
    "                            new_bottom = self._first_occurence[apr_symbol] + self._count_dict[new_bottom + 1][apr_symbol] - 1\n",
    "                    \n",
    "                        top_bottom_update.insert(-1, [new_top, new_bottom, mismatch, count])\n",
    "                        #print('new_top_bottom', new_top, new_bottom)\n",
    "                        #print(top_bottom_update)\n",
    "                                   \n",
    "            else:\n",
    "                new_top_bottom_update = top_bottom_update[0 : -1]\n",
    "                for x in new_top_bottom_update:\n",
    "                    \n",
    "                    top = x[0]\n",
    "                    bottom = x[1]\n",
    "                    for i in range(top, bottom + 1): # D(x)\n",
    "                        # Set the initial backward count.\n",
    "                        count = 0\n",
    "                        last_index = i\n",
    "\n",
    "                        while last_index not in suffix_array_index:\n",
    "\n",
    "                            count += 1\n",
    "                            symbol = self._last_column_str[last_index]\n",
    "                            last_index = self._first_occurence[symbol] + self.count_symbol(symbol, last_index)\n",
    "\n",
    "                        self._start_positions.append(partial_suffix_array[last_index] + count)\n",
    "                \n",
    "                return (self._start_positions)    \n",
    "        return('None')\n",
    "    \n",
    "    # Adding index for each symbol.\n",
    "    def index_symbol_of_column(self, column):\n",
    "       \n",
    "        l = len(column)\n",
    "        col = column[::]\n",
    "        \n",
    "        for x in self._alphabet:\n",
    "            count = 0\n",
    "            for i in range(l):\n",
    "                # Index equals occurence order of symbol.\n",
    "                if col[i] == x:\n",
    "                    count = count + 1\n",
    "                    col[i] = (count, x)\n",
    "                    \n",
    "        return(col) \n",
    "    # Finding index of symbol in the first column coresponds to the same symbol in the last column.\n",
    "    def get_last_to_first(self, last_index):\n",
    "        \n",
    "        first_index = self._index_first_column.index(self._index_last_column[last_index])\n",
    "        \n",
    "        return (first_index)\n",
    "    # Finding index of the symbol occuring at first in the first column.\n",
    "    def get_first_occurence(self):\n",
    "        first_occurence = {}\n",
    "        for x in self._alphabet:\n",
    "            first_occurence.update({x : self._first_column.index(x)})\n",
    "        return(first_occurence)\n",
    "    # Count symbol from 0 to index.\n",
    "    def count_symbol(self, symbol, index):\n",
    "        \n",
    "        return(self._last_column_str.count(symbol, 0, index))\n",
    "    # Construct checkpoints arrays.\n",
    "    def get_check_point_arrays(self, c):\n",
    "        \n",
    "        length = len(self._last_column)\n",
    "        index_check_point = np.arange(0, length + 1, c).tolist()\n",
    "        count_dict = {}\n",
    "\n",
    "        for i in index_check_point: \n",
    "            check_point_arrays = {}\n",
    "            instance_column = self._last_column[0: i]\n",
    "            # Finding number of symbols' occurence from 0 to i.\n",
    "            count_dict.update({i:Counter(instance_column)})\n",
    "        \n",
    "        return(index_check_point, count_dict)   \n",
    "\n",
    "    def divide_pattern(self, pattern, d):\n",
    "        sub_strings = []\n",
    "        k = int(len(pattern)/(d+1))\n",
    "        for i in range(0, d):\n",
    "            sub_strings.append(pattern[i*k: ((i+1)*k)])\n",
    "        sub_strings.append(pattern[d*k::])\n",
    "        return(sub_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0029985904693603516 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 7, 8, 9], [4], [4], [6]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ACATGCTACTTT$'\n",
    "patterns = ['ATT', 'GCC', 'GCTA', 'TATT']\n",
    "pattern = patterns[0]\n",
    "d = 1\n",
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 1)\n",
    "bwt = get_bwt(text)\n",
    "top = 0\n",
    "bottom = len(text) - 1\n",
    "import time\n",
    "c = 1\n",
    "start_time = time.time()\n",
    "starting_positions = InverseBurrowsWheelerTransformNoneRecurrent(bwt).get_approximate_pattern_matching_with_check_points(patterns, c, d, partial_suffix_array, suffix_array_index)\n",
    "#starting_positions = InverseBurrowsWheelerTransformNoneRecurrent(bwt).get_sub_aproximate_string_with_check_points(pattern, top, bottom, d, \n",
    "                                                    #partial_suffix_array, suffix_array_index\n",
    "                                                    #)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "starting_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T$TCAGATTCATC'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of text:  10001\n",
      "Number of patterns:  2000\n"
     ]
    }
   ],
   "source": [
    "with open('D:/Data Science/Data/Mutations/How Do We Locate Disease/Burrows and Wheeler Set Up Checkpoints/dataset_304_10 (4).txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    f.close\n",
    "text = lines[0] + '$'\n",
    "patterns = lines[1:-1][0].split()\n",
    "d = int(lines[-1])\n",
    "partial_suffix_array, suffix_array_index = construct_partial_suffix_arrays(text, 1)\n",
    "bwt = get_bwt(text)\n",
    "print('The length of text: ', len(text))\n",
    "print('Number of patterns: ', len(patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 53.70004391670227 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "c = 100\n",
    "start_time = time.time()\n",
    "starting_positions = InverseBurrowsWheelerTransformNoneRecurrent(bwt).get_approximate_pattern_matching_with_check_points(patterns, c, d, partial_suffix_array, suffix_array_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 9, 11, 13, 15, 21, 27, 31, 40, 40, 44, 50, 54, 54, 57, 61, 62, 69, 76, 95, 100, 102, 103, 106, 107, 119, 124, 129, 139, 145, 157, 186, 192, 192, 195, 197, 200, 212, 223, 223, 224, 229, 243, 244, 245, 261, 264, 270, 274, 287, 301, 304, 304, 306, 312, 313, 323, 327, 329, 330, 340, 346, 357, 359, 359, 359, 363, 364, 372, 379, 386, 388, 394, 403, 404, 409, 420, 428, 440, 441, 453, 458, 459, 460, 461, 462, 465, 473, 481, 485, 497, 509, 513, 513, 527, 530, 535, 536, 537, 539, 541, 547, 547, 553, 557, 562, 566, 568, 573, 582, 592, 593, 598, 601, 603, 607, 635, 640, 649, 652, 660, 661, 667, 671, 682, 696, 708, 709, 722, 726, 734, 740, 740, 746, 747, 764, 765, 772, 779, 781, 784, 789, 790, 803, 809, 821, 837, 839, 841, 844, 845, 849, 849, 850, 853, 853, 858, 867, 869, 882, 887, 902, 907, 910, 911, 912, 919, 919, 927, 929, 931, 935, 940, 941, 948, 953, 960, 960, 966, 980, 981, 988, 990, 991, 993, 994, 996, 996, 1002, 1005, 1008, 1021, 1022, 1023, 1023, 1024, 1032, 1033, 1050, 1056, 1061, 1077, 1083, 1089, 1101, 1103, 1104, 1105, 1113, 1116, 1126, 1131, 1139, 1147, 1149, 1169, 1176, 1179, 1180, 1185, 1185, 1187, 1188, 1193, 1199, 1200, 1203, 1206, 1209, 1213, 1214, 1216, 1219, 1226, 1231, 1234, 1242, 1255, 1261, 1267, 1268, 1270, 1278, 1283, 1285, 1287, 1289, 1290, 1290, 1294, 1306, 1306, 1307, 1314, 1325, 1327, 1327, 1329, 1342, 1349, 1354, 1359, 1360, 1363, 1371, 1376, 1378, 1386, 1396, 1403, 1408, 1426, 1431, 1434, 1435, 1438, 1444, 1453, 1459, 1467, 1470, 1470, 1471, 1472, 1477, 1483, 1492, 1492, 1499, 1499, 1501, 1507, 1513, 1522, 1528, 1533, 1533, 1534, 1535, 1547, 1547, 1548, 1567, 1568, 1568, 1569, 1572, 1580, 1584, 1594, 1595, 1597, 1602, 1609, 1624, 1631, 1632, 1638, 1645, 1656, 1663, 1675, 1675, 1676, 1680, 1685, 1694, 1695, 1697, 1708, 1711, 1715, 1723, 1723, 1724, 1728, 1730, 1732, 1732, 1734, 1734, 1746, 1749, 1758, 1760, 1762, 1775, 1780, 1797, 1800, 1804, 1815, 1817, 1823, 1836, 1838, 1838, 1841, 1843, 1846, 1858, 1859, 1867, 1875, 1886, 1897, 1898, 1899, 1913, 1915, 1923, 1924, 1924, 1927, 1932, 1933, 1933, 1934, 1945, 1948, 1949, 1951, 1972, 1972, 1976, 1979, 1982, 1983, 1986, 1988, 1990, 2006, 2007, 2008, 2009, 2018, 2023, 2026, 2030, 2039, 2044, 2047, 2048, 2080, 2083, 2089, 2090, 2091, 2094, 2097, 2102, 2116, 2117, 2122, 2123, 2128, 2132, 2139, 2156, 2161, 2163, 2165, 2172, 2178, 2180, 2184, 2187, 2191, 2200, 2201, 2203, 2225, 2232, 2237, 2238, 2249, 2254, 2267, 2271, 2271, 2274, 2277, 2280, 2280, 2281, 2284, 2289, 2297, 2307, 2313, 2325, 2328, 2332, 2337, 2339, 2342, 2345, 2359, 2364, 2366, 2368, 2369, 2373, 2375, 2378, 2378, 2406, 2413, 2413, 2416, 2421, 2434, 2436, 2443, 2447, 2449, 2450, 2463, 2469, 2489, 2496, 2501, 2504, 2519, 2531, 2542, 2560, 2563, 2567, 2568, 2573, 2581, 2591, 2600, 2601, 2615, 2626, 2631, 2634, 2642, 2643, 2646, 2647, 2660, 2666, 2670, 2677, 2679, 2680, 2685, 2686, 2690, 2697, 2697, 2698, 2724, 2728, 2729, 2738, 2743, 2746, 2748, 2753, 2755, 2757, 2761, 2764, 2767, 2768, 2780, 2781, 2784, 2788, 2797, 2798, 2799, 2810, 2810, 2812, 2814, 2819, 2824, 2842, 2845, 2848, 2855, 2856, 2868, 2874, 2880, 2887, 2890, 2892, 2897, 2899, 2907, 2929, 2929, 2931, 2934, 2935, 2942, 2955, 2969, 2974, 2982, 2982, 2984, 2989, 2989, 2990, 3000, 3001, 3014, 3019, 3019, 3020, 3024, 3034, 3037, 3038, 3040, 3047, 3054, 3057, 3058, 3075, 3076, 3078, 3079, 3092, 3095, 3097, 3099, 3101, 3106, 3121, 3123, 3125, 3132, 3134, 3134, 3143, 3144, 3146, 3147, 3151, 3153, 3159, 3161, 3161, 3167, 3168, 3170, 3171, 3181, 3183, 3185, 3186, 3186, 3187, 3190, 3192, 3202, 3203, 3209, 3215, 3217, 3221, 3223, 3236, 3243, 3245, 3248, 3248, 3250, 3255, 3258, 3267, 3268, 3282, 3287, 3293, 3297, 3300, 3306, 3310, 3313, 3323, 3325, 3330, 3332, 3334, 3338, 3339, 3339, 3342, 3343, 3345, 3351, 3357, 3358, 3359, 3362, 3372, 3373, 3375, 3376, 3380, 3385, 3387, 3388, 3396, 3396, 3400, 3400, 3402, 3407, 3407, 3413, 3414, 3424, 3432, 3435, 3445, 3447, 3452, 3465, 3476, 3481, 3488, 3490, 3490, 3494, 3498, 3500, 3502, 3502, 3521, 3532, 3539, 3545, 3550, 3551, 3553, 3557, 3559, 3564, 3583, 3596, 3601, 3605, 3606, 3607, 3611, 3614, 3623, 3626, 3627, 3633, 3639, 3639, 3640, 3655, 3656, 3665, 3675, 3677, 3697, 3697, 3721, 3729, 3738, 3763, 3769, 3781, 3787, 3787, 3799, 3803, 3812, 3817, 3818, 3823, 3824, 3828, 3838, 3853, 3860, 3861, 3869, 3873, 3875, 3875, 3879, 3885, 3887, 3888, 3889, 3897, 3905, 3906, 3908, 3908, 3921, 3924, 3924, 3924, 3926, 3926, 3936, 3937, 3941, 3945, 3949, 3957, 3961, 3963, 3963, 3971, 3975, 3990, 3995, 3995, 4002, 4006, 4009, 4019, 4022, 4026, 4027, 4038, 4041, 4041, 4047, 4049, 4055, 4055, 4056, 4059, 4067, 4070, 4070, 4071, 4076, 4088, 4094, 4108, 4127, 4141, 4144, 4145, 4153, 4154, 4156, 4157, 4161, 4166, 4169, 4173, 4175, 4176, 4176, 4184, 4195, 4198, 4202, 4206, 4212, 4213, 4220, 4223, 4232, 4233, 4235, 4238, 4239, 4242, 4250, 4251, 4255, 4255, 4263, 4284, 4286, 4292, 4292, 4295, 4296, 4302, 4310, 4313, 4320, 4328, 4328, 4338, 4351, 4362, 4363, 4367, 4368, 4373, 4382, 4383, 4386, 4386, 4393, 4394, 4406, 4435, 4437, 4437, 4438, 4439, 4441, 4451, 4452, 4452, 4458, 4462, 4464, 4483, 4499, 4503, 4504, 4517, 4519, 4520, 4532, 4544, 4544, 4552, 4556, 4559, 4559, 4562, 4569, 4574, 4582, 4591, 4595, 4599, 4604, 4612, 4622, 4628, 4632, 4641, 4643, 4660, 4661, 4670, 4673, 4679, 4685, 4688, 4692, 4701, 4707, 4708, 4708, 4726, 4727, 4727, 4743, 4745, 4745, 4746, 4752, 4755, 4758, 4759, 4760, 4764, 4764, 4765, 4767, 4769, 4771, 4777, 4777, 4782, 4803, 4814, 4819, 4824, 4826, 4831, 4835, 4841, 4841, 4853, 4856, 4865, 4868, 4869, 4875, 4878, 4879, 4880, 4880, 4886, 4889, 4891, 4896, 4901, 4903, 4908, 4913, 4917, 4934, 4936, 4950, 4951, 4967, 4971, 4973, 4977, 4986, 4993, 4995, 5000, 5002, 5003, 5005, 5011, 5027, 5031, 5040, 5040, 5044, 5049, 5057, 5059, 5064, 5068, 5069, 5077, 5080, 5083, 5087, 5088, 5089, 5092, 5094, 5094, 5095, 5095, 5096, 5096, 5096, 5097, 5102, 5119, 5120, 5126, 5134, 5142, 5142, 5151, 5158, 5166, 5183, 5192, 5201, 5202, 5217, 5225, 5225, 5234, 5238, 5239, 5243, 5244, 5247, 5250, 5251, 5251, 5253, 5255, 5258, 5258, 5263, 5268, 5268, 5270, 5271, 5273, 5277, 5282, 5288, 5291, 5293, 5297, 5298, 5301, 5306, 5313, 5317, 5322, 5323, 5323, 5324, 5328, 5340, 5349, 5352, 5356, 5360, 5360, 5361, 5364, 5365, 5370, 5378, 5381, 5382, 5386, 5389, 5391, 5392, 5396, 5398, 5399, 5410, 5413, 5420, 5424, 5446, 5452, 5457, 5466, 5467, 5479, 5497, 5497, 5500, 5508, 5509, 5509, 5512, 5518, 5521, 5523, 5525, 5531, 5534, 5541, 5547, 5548, 5557, 5566, 5567, 5567, 5572, 5578, 5584, 5586, 5594, 5602, 5602, 5607, 5611, 5617, 5619, 5620, 5621, 5626, 5630, 5634, 5641, 5641, 5642, 5644, 5649, 5649, 5664, 5670, 5670, 5674, 5677, 5682, 5693, 5698, 5703, 5708, 5714, 5715, 5715, 5716, 5720, 5721, 5744, 5746, 5751, 5753, 5753, 5756, 5762, 5771, 5771, 5775, 5785, 5785, 5788, 5788, 5799, 5801, 5804, 5807, 5807, 5812, 5827, 5828, 5834, 5841, 5848, 5849, 5852, 5857, 5858, 5859, 5871, 5877, 5882, 5886, 5889, 5891, 5899, 5909, 5912, 5916, 5916, 5917, 5924, 5939, 5940, 5948, 5949, 5949, 5954, 5965, 5967, 5968, 5971, 5975, 5983, 5993, 5997, 6000, 6012, 6019, 6023, 6026, 6032, 6041, 6056, 6068, 6069, 6077, 6079, 6086, 6091, 6097, 6097, 6100, 6106, 6110, 6111, 6117, 6118, 6134, 6138, 6144, 6149, 6158, 6164, 6165, 6165, 6174, 6178, 6178, 6178, 6187, 6191, 6195, 6199, 6200, 6201, 6210, 6215, 6218, 6222, 6234, 6240, 6241, 6245, 6257, 6261, 6265, 6268, 6279, 6280, 6287, 6292, 6295, 6300, 6304, 6305, 6305, 6306, 6309, 6319, 6323, 6325, 6326, 6328, 6330, 6343, 6343, 6346, 6352, 6352, 6355, 6357, 6368, 6368, 6375, 6376, 6376, 6379, 6380, 6382, 6387, 6391, 6392, 6415, 6415, 6427, 6428, 6436, 6438, 6449, 6464, 6472, 6476, 6484, 6484, 6495, 6500, 6503, 6503, 6509, 6513, 6520, 6524, 6524, 6529, 6532, 6547, 6548, 6549, 6550, 6552, 6554, 6563, 6567, 6567, 6570, 6574, 6581, 6583, 6606, 6622, 6623, 6623, 6626, 6628, 6633, 6638, 6639, 6647, 6650, 6655, 6656, 6659, 6659, 6665, 6670, 6670, 6676, 6677, 6679, 6688, 6694, 6699, 6700, 6703, 6704, 6709, 6734, 6748, 6754, 6760, 6781, 6807, 6809, 6810, 6812, 6819, 6820, 6823, 6835, 6838, 6841, 6848, 6849, 6858, 6859, 6866, 6867, 6871, 6872, 6874, 6885, 6886, 6888, 6919, 6920, 6938, 6942, 6946, 6955, 6957, 6958, 6958, 6962, 6962, 6969, 6978, 6978, 6990, 7000, 7014, 7022, 7037, 7038, 7043, 7052, 7054, 7064, 7073, 7081, 7086, 7088, 7092, 7094, 7101, 7109, 7111, 7118, 7129, 7130, 7132, 7135, 7136, 7140, 7167, 7172, 7188, 7195, 7224, 7233, 7234, 7239, 7244, 7246, 7247, 7248, 7248, 7249, 7254, 7255, 7256, 7267, 7270, 7275, 7280, 7284, 7285, 7293, 7297, 7306, 7309, 7312, 7318, 7322, 7327, 7330, 7341, 7352, 7355, 7363, 7366, 7366, 7366, 7367, 7370, 7377, 7379, 7380, 7388, 7391, 7392, 7392, 7393, 7397, 7403, 7403, 7414, 7416, 7418, 7421, 7423, 7427, 7431, 7448, 7449, 7455, 7455, 7459, 7461, 7465, 7476, 7479, 7483, 7485, 7488, 7497, 7499, 7513, 7513, 7517, 7518, 7520, 7520, 7532, 7532, 7533, 7549, 7552, 7558, 7564, 7567, 7581, 7586, 7589, 7591, 7597, 7601, 7607, 7619, 7624, 7625, 7625, 7627, 7632, 7652, 7659, 7666, 7667, 7697, 7701, 7702, 7702, 7707, 7707, 7710, 7714, 7716, 7717, 7721, 7724, 7742, 7743, 7745, 7755, 7768, 7771, 7772, 7774, 7784, 7787, 7789, 7792, 7794, 7797, 7798, 7816, 7823, 7824, 7826, 7833, 7834, 7836, 7841, 7846, 7846, 7855, 7863, 7864, 7865, 7873, 7874, 7878, 7885, 7888, 7888, 7892, 7894, 7895, 7898, 7904, 7905, 7912, 7915, 7925, 7931, 7933, 7939, 7940, 7942, 7944, 7945, 7947, 7948, 7948, 7961, 7963, 7964, 7973, 7985, 7988, 7993, 7996, 7997, 8000, 8009, 8018, 8018, 8025, 8032, 8038, 8038, 8039, 8041, 8047, 8050, 8071, 8077, 8086, 8091, 8094, 8096, 8107, 8113, 8113, 8114, 8115, 8115, 8116, 8119, 8120, 8123, 8124, 8126, 8132, 8137, 8141, 8143, 8147, 8154, 8166, 8173, 8174, 8175, 8179, 8181, 8193, 8197, 8204, 8206, 8208, 8220, 8228, 8241, 8242, 8243, 8244, 8256, 8265, 8273, 8273, 8277, 8290, 8294, 8300, 8300, 8300, 8305, 8307, 8309, 8311, 8311, 8316, 8316, 8319, 8322, 8327, 8331, 8335, 8343, 8353, 8366, 8366, 8387, 8397, 8400, 8405, 8411, 8415, 8417, 8421, 8424, 8438, 8448, 8449, 8452, 8458, 8467, 8469, 8470, 8476, 8488, 8494, 8496, 8498, 8507, 8510, 8520, 8533, 8536, 8542, 8545, 8546, 8546, 8551, 8552, 8554, 8555, 8560, 8563, 8565, 8570, 8572, 8573, 8576, 8579, 8580, 8581, 8589, 8594, 8596, 8598, 8613, 8614, 8614, 8616, 8621, 8637, 8637, 8642, 8643, 8648, 8650, 8657, 8664, 8678, 8679, 8679, 8685, 8685, 8687, 8706, 8718, 8722, 8726, 8727, 8727, 8729, 8730, 8730, 8731, 8739, 8740, 8750, 8753, 8767, 8768, 8773, 8774, 8789, 8794, 8797, 8808, 8826, 8829, 8831, 8833, 8844, 8847, 8855, 8856, 8861, 8861, 8864, 8865, 8876, 8883, 8884, 8896, 8896, 8896, 8901, 8904, 8910, 8913, 8931, 8931, 8932, 8941, 8948, 8952, 8954, 8965, 8967, 8969, 8970, 8972, 8975, 8986, 8987, 8988, 8997, 9011, 9011, 9036, 9037, 9041, 9044, 9046, 9066, 9066, 9068, 9073, 9090, 9091, 9092, 9093, 9093, 9094, 9095, 9107, 9112, 9119, 9128, 9132, 9135, 9144, 9145, 9147, 9152, 9153, 9156, 9161, 9170, 9171, 9175, 9176, 9190, 9195, 9195, 9197, 9209, 9230, 9235, 9243, 9246, 9255, 9256, 9261, 9263, 9265, 9268, 9280, 9281, 9282, 9288, 9289, 9293, 9299, 9308, 9315, 9318, 9319, 9326, 9329, 9333, 9333, 9343, 9344, 9346, 9351, 9352, 9353, 9364, 9369, 9371, 9372, 9389, 9394, 9404, 9404, 9406, 9416, 9427, 9431, 9432, 9435, 9438, 9442, 9452, 9458, 9461, 9477, 9479, 9480, 9482, 9484, 9484, 9494, 9495, 9496, 9497, 9506, 9518, 9520, 9530, 9530, 9531, 9534, 9562, 9563, 9570, 9575, 9590, 9597, 9598, 9605, 9608, 9614, 9625, 9626, 9630, 9652, 9659, 9664, 9666, 9670, 9673, 9676, 9677, 9681, 9684, 9686, 9687, 9695, 9698, 9702, 9702, 9710, 9712, 9714, 9717, 9723, 9725, 9728, 9759, 9768, 9769, 9776, 9776, 9782, 9782, 9784, 9790, 9799, 9801, 9810, 9819, 9821, 9823, 9824, 9825, 9830, 9833, 9835, 9852, 9860, 9866, 9868, 9878, 9880, 9884, 9884, 9887, 9887, 9893, 9894, 9902, 9903, 9904, 9905, 9910, 9912, 9932, 9935, 9939]\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "for x in starting_positions:\n",
    "    for el in x:\n",
    "        start_positions.append(el)\n",
    "\n",
    "print(sorted(start_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hidden Markov Models<a name = 'hmm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.144</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.232</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.099</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A      B      C      D\n",
       "A  0.144  0.219  0.383  0.254\n",
       "B  0.232  0.217  0.062  0.489\n",
       "C  0.107  0.290  0.302  0.301\n",
       "D  0.099  0.157  0.258  0.486"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = pd.read_csv('D:/Data Science/Data/Mutations/How Do We Locate Disease/Viterbi/trans.txt', sep = '\\s+')\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.059</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.184</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y      z\n",
       "A  0.600  0.126  0.274\n",
       "B  0.059  0.591  0.350\n",
       "C  0.184  0.575  0.241\n",
       "D  0.442  0.440  0.118"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emis = pd.read_csv('D:/Data Science/Data/Mutations/How Do We Locate Disease/Viterbi/emis.txt', sep = '\\s+')\n",
    "emis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = 'yyxxyxxxxyxyxxzyzyyxxyyxzyxyyxzzzzzxzzzzzyzxxxzzxzzxyyxyyxzyxxyzxzzzxzxyzzxxzyzyyyzxxxxyzxyxyyzxzxxx'\n",
    "states = 'ABCD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = {}\n",
    "emission = {}\n",
    "\n",
    "for x in trans.index:\n",
    "    tr = {}\n",
    "    for y in trans.columns:\n",
    "        tr.update({y : trans.loc[x, y]})\n",
    "    transition.update({x : tr})    \n",
    "    \n",
    "for x in emis.index:\n",
    "    em = {}\n",
    "    for y in emis.columns:\n",
    "        em.update({y : emis.loc[x, y]})\n",
    "    emission.update({x : em}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viterbi_algorithm(Sigma, states, transition, emission):\n",
    "    \n",
    "    n =  len(Sigma)\n",
    "    source_prop = 1/len(states)\n",
    "    back_track = {}\n",
    "    path = []\n",
    "    s = {}\n",
    "    \n",
    "    # Finding s_{k, source}\n",
    "    for k in states:\n",
    "        s.update({k : {0 : source_prop*emission[k][Sigma[0]]}})\n",
    "    # Finding s_{k, i}\n",
    "    for i in range(1, n):\n",
    "        sub_back_track = {}\n",
    "        \n",
    "        for k in states:    \n",
    "            prop = []\n",
    "            _emis = emission[k][Sigma[i]]\n",
    "            \n",
    "            for l in states:\n",
    "                prop.append(s[l][i-1]*transition[l][k]*_emis)\n",
    "               \n",
    "            s[k].update({i : max(prop)})\n",
    "            \n",
    "            sub_back_track.update({k : states[np.argmax(prop)]})\n",
    "        # Get back_tracking \n",
    "        back_track.update({i : sub_back_track})\n",
    "    \n",
    "    prop = []    \n",
    "    \n",
    "    # Get the last state.\n",
    "    for l in states:\n",
    "        prop.append(s[l][n-1])\n",
    "    state = states[np.argmax(prop)]\n",
    "    \n",
    "    # Find the path of states.\n",
    "    path.append(state)\n",
    "\n",
    "    for i in reversed(range(1, n)):\n",
    "        state = back_track[i][state] \n",
    "        path.append(state)\n",
    "        \n",
    "    return(''.join(reversed(path)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BDDDDDDDDDDDDDCCBDDDDDDDDDDDDDCBACBDCBBBACBDDDCBDCBDDDDDDDDDDDCBDCBBDDDDCBDDCCBDDCBDDDDCBDDDDCBDDDDD'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_viterbi_algorithm(Sigma, states, transition, emission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
